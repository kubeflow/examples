'use strict';
const FL =
`
from typing import NamedTuple
import kfp
from kfp import dsl
from kfp.components import func_to_container_op
from kubernetes.client.models import V1ContainerPort

NUM_OF_CLIENTS = %s

def server(NUM_OF_CLIENTS:int):
    import json
    import pandas as pd
    import numpy as np
    import pickle
    import threading
    import time
    import tensorflow as tf
    from flask import Flask, jsonify,request
    import os
    
    app = Flask(__name__)
    clients_local_count = []
    scaled_local_weight_list = []
    global_value = { #Share variable
                    'last_run_statue' : False, #last run finish or not
                    'data_statue' : None,      #global_count finish or not
                    'global_count' : None,
                    'scale_statue' : None,
                    'weight_statue' : None,
                    'average_weights' : None,
                    'shutdown' : 0}
    
    
    
    init_lock = threading.Lock()
    clients_local_count_lock = threading.Lock()
    scaled_local_weight_list_lock = threading.Lock()
    cal_weight_lock = threading.Lock()
    shutdown_lock = threading.Lock()
    
    @app.before_request
    def before_request():
        print('get request')
        
        
    @app.route('/data', methods=['POST'])
    def flask_server():
        with init_lock:  #check last run is finish and init varible
            
            while True:
                
                if(len(clients_local_count)==0 and global_value['last_run_statue'] == False):#init the variable by first client enter
                    global_value['last_run_statue'] = True
                    global_value['data_statue'] = False
                    global_value['scale_statue'] = False
                    global_value['weight_statue'] = False
                    break
                
                elif(global_value['last_run_statue'] == True):
                    break
                time.sleep(3)
        
        local_count = int(request.form.get('local_count'))          #get data
        bs = int(request.form.get('bs'))
        local_weight = json.loads(request.form.get('local_weight'))
        local_weight = [np.array(lst) for lst in local_weight]
        

        
        
        def scale_model_weights(weight, scalar):
            weight_final = []
            steps = len(weight)
            for i in range(steps):
                weight_final.append(scalar * weight[i])
            return weight_final
        def sum_scaled_weights(scaled_weight_list):
            
            avg_grad = list()
            #get the average grad accross all client gradients
            for grad_list_tuple in zip(*scaled_weight_list):
                layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
                avg_grad.append(layer_mean)

            return avg_grad
        
        with clients_local_count_lock:
            clients_local_count.append(int(local_count))
            
        with scaled_local_weight_list_lock:
            while True:
                
                if (len(clients_local_count) == NUM_OF_CLIENTS and global_value['data_statue'] != True):
                    global_value['last_run_statue'] = False
                    sum_of_local_count=sum(clients_local_count)
                    
                    
                    global_value['global_count'] = sum_of_local_count     
                    
                    scaling_factor=local_count/global_value['global_count']
                    scaled_weights = scale_model_weights(local_weight, scaling_factor)
                    scaled_local_weight_list.append(scaled_weights)
                    
                    global_value['scale_statue'] = True 
                    global_value['data_statue'] = True
                    break
                elif (global_value['data_statue'] == True and global_value['scale_statue'] == True):
                    scaling_factor=local_count/global_value['global_count']
                    scaled_weights =scale_model_weights(local_weight, scaling_factor)
                    scaled_local_weight_list.append(scaled_weights)
        
                    break
                time.sleep(1)
               
        with cal_weight_lock:
            
            while True:
                if(len(scaled_local_weight_list) == NUM_OF_CLIENTS and global_value['weight_statue'] != True):
                    
                    global_value['average_weights'] = sum_scaled_weights(scaled_local_weight_list)
                    global_value['weight_statue'] = True
                    global_value['average_weights'] = json.dumps([np.array(w).tolist() for w in global_value['average_weights']])
                    
                    break
                    
                elif(global_value['weight_statue'] == True):
                    
                    break
                
                time.sleep(1)
                
                
        clients_local_count.clear()
        scaled_local_weight_list.clear()
        
        return jsonify({'result': (global_value['average_weights'])})
        
    
    @app.route('/shutdown', methods=['GET'])
    def shutdown_server():
        global_value['shutdown'] +=1 
        with shutdown_lock:
            while True:
                if(global_value['shutdown'] == NUM_OF_CLIENTS):
                    os._exit(0)
                    return 'Server shutting down...'
                time.sleep(1)
    
    
    app.run(host="0.0.0.0", port=8080)
    
server_op=func_to_container_op(server,base_image='tensorflow/tensorflow',packages_to_install=['flask','pandas'])

def client(batch:int,num_of_clients:int) -> NamedTuple('Outputs', [("last_accuracy",float)]):
    import json
    import requests
    import time
    import pandas as pd
    import numpy as np
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.optimizers import SGD
    from tensorflow.keras import backend as K
    
    def split_and_get_batch(data, labels, x, batch_index):
        
        batch_size = len(data) // x

        
        data_batches = np.array_split(data, x)
        label_batches = np.array_split(labels, x)

        
        selected_data_batch = data_batches[batch_index]
        selected_label_batch = label_batches[batch_index]

        return selected_data_batch, selected_label_batch
    
    
    normal_url = 'https://drive.google.com/u/0/uc?id=1TQHKkP6yzuhcxw_JCtby9jQwY2AMLiNi&export=download'
    abnormal_url = 'https://drive.google.com/uc?export=download&id=1i22tQI2vib0fsd1wwVP1tEydmGEksmpy'

    normal_data = pd.read_csv(normal_url)
    abnormal_data = pd.read_csv(abnormal_url)

    num_features = len(normal_data.columns)
    print(num_features)

    normal_label = np.array([[1, 0]] * len(normal_data))
    abnormal_label = np.array([[0, 1]] * len(abnormal_data))

    data = np.vstack((normal_data, abnormal_data))
    data_label = np.vstack((normal_label, abnormal_label))

    shuffler = np.random.permutation(len(data))
    data = data[shuffler]
    data_label = data_label[shuffler]

    data = data.reshape(len(data), num_features, 1)
    data_label = data_label.reshape(len(data_label), 2)

    selected_data, selected_labels = split_and_get_batch(data,data_label,num_of_clients,batch-1)
    full_data = list(zip(selected_data,selected_labels))
    class SimpleMLP:
        @staticmethod
        def build(shape, classes):
            model = Sequential()
            model.add(Conv1D(filters=4, kernel_size=3, input_shape=(17,1)))
            model.add(MaxPooling1D(3))
            model.add(Flatten())
            model.add(Dense(8, activation="relu"))
            model.add(Dense(2, activation = 'softmax'))

            return model
    
    
    
    print('data len= ',len(full_data))
    def batch_data(data_shard, bs=32):
    
        #seperate shard into data and labels lists
        data, label = zip(*data_shard)
        dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))
        return dataset.shuffle(len(label)).batch(bs)
    
    dataset=batch_data(full_data)
    #print(dataset)
    
    bs = next(iter(dataset))[0].shape[0]
    local_count = tf.data.experimental.cardinality(dataset).numpy()*bs
    
    
    loss='categorical_crossentropy'
    metrics = ['accuracy']
    optimizer = 'adam'
    
    
    smlp_model = SimpleMLP()
    
    server_url="http://http-service:5000/data"
    for comm_round in range(1):
        print('The ',comm_round+1, 'round')
        client_model = smlp_model.build(17, 1)
        client_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        
        if(comm_round == 0):
            history = client_model.fit(dataset, epochs=5, verbose=1)
        else:
            client_model.set_weights(avg_weight)
            history = client_model.fit(dataset, epochs=5, verbose=1)
        
        local_weight = client_model.get_weights()
        local_weight = [np.array(w).tolist() for w in local_weight]
        
        client_data = {"local_count": local_count,'bs': bs, 'local_weight': json.dumps(local_weight)}
        
        while True:
            try:
                weight = (requests.post(server_url,data=client_data))
                
                if weight.status_code == 200:
                    print(f"exist")

                    break
                else:
                    print(f"server error")

            except requests.exceptions.RequestException:

                print(f"not exist")
                
            time.sleep(5)
            
        data = weight.json()
        avg_weight = data.get('result')
        avg_weight = json.loads(avg_weight)
        avg_weight = [np.array(lst) for lst in avg_weight]
        
    shutdown_url="http://http-service:5000/shutdown"    
    try:
        response = requests.get(shutdown_url)
    except requests.exceptions.ConnectionError:
        print('already shutdown')
    last_accuracy = history.history['accuracy'][-1]
    print(last_accuracy)
    return([last_accuracy])

client_op=func_to_container_op(client,base_image='tensorflow/tensorflow',packages_to_install=['requests','pandas'])
from typing import List, NamedTuple

def show_results(test_acc: List[float]) -> NamedTuple('Outputs', [('test_accuracy', str)]):
    print("Original test_acc:", len(test_acc))
    
    
    
    return ([test_acc])

show_results_op = func_to_container_op(show_results)


def generate_clients(n):
    client_tasks = []
    for i in range(n):
        client_task = globals()['client_task_' + str(i + 1)] = client_op(i + 1, n)
        client_task.set_cpu_request('0.2').set_cpu_limit('0.2')
        client_tasks.append(client_task)
    
    globals()['show_results_task'] = show_results_op([ct.outputs['last_accuracy'] for ct in client_tasks]).set_cpu_request('0.2').set_cpu_limit('0.2')
        
@dsl.pipeline(
    name='FL test'
    )
def fl_pipeline(namespace='kubeflow-user-thu01'):
    global NUM_OF_CLIENTS
    service = dsl.ResourceOp(
        name='http-service',
        k8s_resource={
            'apiVersion': 'v1',
            'kind': 'Service',
            'metadata': {
                'name': 'http-service'
            },
            'spec': {
                'selector': {
                    'app': 'http-service'
                },
                'ports': [
                    {
                        'protocol': 'TCP',
                        'port': 5000,
                        'targetPort': 8080
                    }
                ]
            }
        }
    )#service_end
    server_task=server_op(NUM_OF_CLIENTS)
    server_task.add_pod_label('app', 'http-service')
    server_task.add_port(V1ContainerPort(name='my-port', container_port=8080))
    server_task.set_cpu_request('0.2').set_cpu_limit('0.2')
    server_task.after(service)
    
    delete_service = dsl.ResourceOp(
        name='delete-service',
        k8s_resource={
            'apiVersion': 'v1',
            'kind': 'Service',
            'metadata': {
                'name': 'http-service'
            },
            'spec': {
                'selector': {
                    'app': 'http-service'
                },
                'ports': [
                    {
                        'protocol': 'TCP',
                        'port': 80,
                        'targetPort': 8080
                    }
                ],
                'type': 'NodePort'  
            }
        },
        action="delete" #刪除
    ).after(server_task)
    
    generate_clients(NUM_OF_CLIENTS)
         
kfp.compiler.Compiler().compile(fl_pipeline, 'fl_node_pipeline.yaml')  


import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = "https://ai4edu.thu01.footprint-ai.com"
username = "thu01"
password = "mB(-B28f"

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True

namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for only_randomforest."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile="fl_node_pipeline.yaml"
    name='pipeline-' + random_suffix()
    description="This is a only_randomforest pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'FL' + random_suffix()
    description = "This is a only_randomforest run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)

    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    


    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)
            #print(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions'] # Comfirm completion.
                    
            except KeyError:
                nodes = {}
                conditions = []

            output_value = None
            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False
            
            '''''
            def find_all_keys(node):
                if isinstance(node, dict):
                    for key in node.keys():
                        print("Key:", key)
                        find_all_keys(node[key])
                elif isinstance(node, list):
                    for item in node:
                        find_all_keys(item)

            # Call the function with your JSON data
            find_all_keys(output)
            '''''

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Pipeline is still running. Waiting...")
            time.sleep(polling_interval-1)
    
    found_final_pvc_name = False  # Add a variable to track if the PVC name has been found

    def find_final_pvc_name(node):
        global found_final_pvc_name  # Declare the variable as global

        if not found_final_pvc_name:  # If the PVC name has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'mypvc-name':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_final_pvc_name = True  # Set to True after finding the PVC name
                                print("mypvc-name:", value)
                                return value
                for key, value in node.items():
                    result = find_final_pvc_name(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_final_pvc_name(item)
                    if result:
                        return result

        return None
    
    find_final_pvc_name(output)  # Call the function to find final_pvc_name


    found_model_func_accuracy = False

    def find_model_func_accuracy(node):
        global NUM_OF_CLIENTS
        global found_model_func_accuracy  # Declare the variable as global

        if not found_model_func_accuracy:  # If the model-func-accuracy has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        for _ in range(NUM_OF_CLIENTS):
                            if 'name' in parameter and f'last_accuracy' in parameter['name']:
                                value = parameter.get('value')
                                if value and not value.startswith('{{') and not value.endswith('}}'):
                                    found_model_func_accuracy = True  # Set to True after finding model-func-accuracy
                                    print(value)
                                    return value

                            

                for key, value in node.items():
                    result = find_model_func_accuracy(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_model_func_accuracy(item)
                    if result:
                        return result

        return None
    
    find_model_func_accuracy(output)
`;
const IMPORTS =
`
from typing import NamedTuple
import kfp
from kfp import dsl
from kfp.components import func_to_container_op, InputPath, OutputPath
import kfp.components as components
import datetime
import os
`;


const DATA_UPLOAD =
`
from minio import Minio
from minio.error import S3Error
import os 
import csv

host = os.getenv("MINIO_HOST")
username = os.getenv("MINIO_ROOT_USER")
password = os.getenv("MINIO_ROOT_PASSWORD")

      
# Initialize MinIO client
client = Minio(host ,
               access_key=username,
               secret_key=password,
               secure=False)


local_file = "%s"
bucket_name = "data-bucket"
object_name = "%s"
label = %s
unique_labels = set()

with open(local_file, 'r') as f:
    reader = csv.reader(f)
    column_names = next(reader)  
    label_idx = column_names.index(label)
    print("COLUMN_NAMES:", str(column_names))
    for row in reader:
        unique_labels.add(row[label_idx])
    print("Classnames:", str([str(i) for i in unique_labels]))


if not client.bucket_exists(bucket_name):
    client.make_bucket(bucket_name)
try:
    result = client.fput_object(bucket_name, object_name, local_file)
    print("Uploaded", result.object_name, "with ETag:", result.etag)
except S3Error as err:
    print("Error:", err)

`;


const P_DATA_UPLOAD =
`
from minio import Minio
from minio.error import S3Error
import os 


host = os.getenv("MINIO_HOST")
username = os.getenv("MINIO_ROOT_USER")
password = os.getenv("MINIO_ROOT_PASSWORD")

      
# Initialize MinIO client
client = Minio(host ,
               access_key=username,
               secret_key=password,
               secure=False)


local_file = "%s"
bucket_name = "sample-bucket"
object_name = "%s"

if not client.bucket_exists(bucket_name):
    client.make_bucket(bucket_name)
try:
    result = client.fput_object(bucket_name, object_name, local_file)
    print("Uploaded", result.object_name, "with ETag:", result.etag)
except S3Error as err:
    print("Error:", err)

`;


const DATASETS_CUSTOMIZED =
`
def load_data_func(log_folder:str) -> NamedTuple('Outputs', [('samples', str), ('labels', str)]):
    from typing import NamedTuple
    import os
    import numpy as np
    import pandas as pd
    from io import StringIO
    from minio import Minio
    from minio.error import S3Error

    host = "%s"
    username = "%s"
    password = "%s"

        
    # Initialize MinIO client
    client = Minio(host ,
                access_key=username,
                secret_key=password ,
                secure=False)

    bucket_name = "data-bucket"
    object_name = "%s"


    try:
        response = client.get_object(bucket_name, object_name)
        
        try:
            dataset = response.read().decode('utf-8')
        except UnicodeDecodeError as ude:
            raise ValueError("Failed to decode the data from MinIO") from ude
                
        df = pd.read_csv(StringIO(dataset))

        client.remove_object(bucket_name, object_name)
    except S3Error as err:
        print("Error:", err)
        

    label_column = %s 
    data = df.drop(label_column, axis=1).values
    target = df[label_column].values
    
    
    np.save(os.path.join(log_folder, 'samples.npy'), data)
    np.save(os.path.join(log_folder, 'labels.npy'), target)
    result = NamedTuple('Outputs', [('samples', str), ('labels', str)])
    return result(
        os.path.join(log_folder, 'samples.npy'),
        os.path.join(log_folder, 'labels.npy')
    )
`;


const DATASETS =
`
def load_data_func(log_folder:str) -> NamedTuple('Outputs', [('samples', str), ('labels', str)]):
    from typing import NamedTuple
    import os
    import tensorflow as tf
    import numpy as np
    
    # START_DATASET_CODE
    dataset = tf.keras.datasets.%s
    # END_DATASET_CODE

    (x_train, y_train), (x_test, y_test) = dataset.load_data()
    

    x = np.concatenate((x_train, x_test), axis=0)
    y = np.concatenate((y_train, y_test), axis=0)

    
    
    np.save(os.path.join(log_folder, 'samples.npy'), x)
    np.save(os.path.join(log_folder, 'labels.npy'), y)
    result = NamedTuple('Outputs', [('samples', str), ('labels', str)])
    return result(
        os.path.join(log_folder, 'samples.npy'),
        os.path.join(log_folder, 'labels.npy')
    )
`;


const FILLNANBYVALUE =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    data = pd.DataFrame(samples, columns=col_names[:])  
    data.fillna(value=%s, inplace=True)  
    samples = data.values
`;   

const P_FILLNANBYVALUE =
`
    data = pd.DataFrame(data, columns=col_names[:])
    data.fillna(value=%s, inplace=True)
    data = data.values  
`;

const FILLNANBYMEAN =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    data = pd.DataFrame(samples, columns=col_names[:])  
    data.fillna(value=df.mean(), inplace=True)  # 使用每列的平均值填充該列的NaN值 
    samples = data.values
`;

const P_FILLNANBYMEAN =
`
    data = pd.DataFrame(data, columns=col_names[:])
    data.fillna(value=df.mean(), inplace=True)  # 使用每列的平均值填充該列的NaN值
    data = data.values 
`;

const FILLNANBYMEDIAN =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    data = pd.DataFrame(samples, columns=col_names[:])  
    data.fillna(value=df.median(), inplace=True)  # 使用每列的中位數填充該列的NaN值
    samples = data.values
`;

const P_FILLNANBYMEDIAN =
`
    data = pd.DataFrame(data, columns=col_names[:])
    data.fillna(value=df.median(), inplace=True)  # 使用每列的中位數填充該列的NaN值
    data = data.values
`;

const LABELENCODING =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    process_column = %s
    encoder = LabelEncoder()
    data = pd.DataFrame(samples, columns=col_names[:])  
    target = pd.DataFrame(labels, columns=[label_column_name])
    for column in process_column:
        if column == label_column_name:
            target[column] = encoder.fit_transform(target[column])
        else:
            data[column] = encoder.fit_transform(data[column])

    samples = data.values
    labels = target.values
`;

const P_LABELENCODING =
`
    process_column = %s
    data = pd.DataFrame(data, columns=col_names[:])
    encoder = LabelEncoder()
    for column in process_column:
        data[column] = encoder.fit_transform(data[column])
    data = data.values
`;

const ONEHOTNCODING =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    process_column = %s
    data = pd.DataFrame(samples, columns=col_names[:])  
    target = pd.DataFrame(labels, columns=[label_column_name])
    for column in process_column:
        if column == label_column_name:
            target = pd.get_dummies(target, columns=[column])
        else:
            data = pd.get_dummies(data, columns=[column])

    samples = data.values
    labels = target.values
`;

const P_ONEHOTNCODING =
`
    process_column = %s
    data = pd.DataFrame(data, columns=col_names[:])
    for column in process_column:
        data = pd.get_dummies(data, columns=[column])
    data = data.values
`;

const NORMALIZATION =
`
    scaler = MinMaxScaler(feature_range=(%s, %s))
    x_train = x_train.reshape(x_train.shape[0],-1)
    x_test = x_test.reshape(x_test.shape[0],-1)
    x_train = scaler.fit_transform(x_train)
    x_test = scaler.transform(x_test)
`;



const STANDARDLIZATION =
`   
    scaler = StandardScaler()
    x_train = x_train.reshape(x_train.shape[0],-1)
    x_test = x_test.reshape(x_test.shape[0],-1)
    x_train = scaler.fit_transform(x_train)
    x_test = scaler.transform(x_test)
`;


const DATA_RESHAPE1D =
`
    x_train = x_train.reshape(x_train.shape[0], -1)
    x_test = x_test.reshape(x_test.shape[0], -1)
`;

const P_DATA_RESHAPE1D =
`
    data = data.reshape(data.shape[0], -1)
`;

const DATA_RESHAPE2D =
`
    x_train = x_train.reshape(x_train.shape[0], %s, %s, %s)
    x_test = x_test.reshape(x_test.shape[0], %s, %s, %s)
`;

const P_DATA_RESHAPE2D =
`
    data = data.reshape(data.shape[0],%s,%s,%s)
`;

const DATAPROCESS =
`
def data_process_func(log_folder:str, samples_path: str, labels_path: str) -> NamedTuple('Outputs', [('x_train', str), ('y_train', str), ('x_test', str), ('y_test', str)]):
    from typing import NamedTuple
    import os
    import tensorflow as tf
    import numpy as np
    from sklearn.model_selection import train_test_split
    import pandas as pd
    from sklearn.preprocessing import LabelEncoder
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.preprocessing import StandardScaler
    from minio import Minio
    from minio.error import S3Error
    import pickle
    from io import BytesIO

    samples = np.load(samples_path)
    labels = np.load(labels_path)
    scaler = False

    %s
    %s
    %s
    
    x_train, x_test, y_train, y_test = train_test_split(samples, labels, test_size=%s, random_state=42)

    
    %s
    %s
    if scaler:
        serialized_scaler = pickle.dumps(scaler)
    
        host = "%s"
        username = "%s"
        password = "%s"
    
          
        # Initialize MinIO client
        client = Minio(host ,
                    access_key=username,
                    secret_key=password,
                    secure=False)
    
    
        
        bucket_name = "scalers"    
    
        if not client.bucket_exists(bucket_name):
            client.make_bucket(bucket_name)
        try:
            result = client.put_object(bucket_name, 'scaler.pkl', BytesIO(serialized_scaler), len(serialized_scaler))
            print("Uploaded", result.object_name, "with ETag:", result.etag)
        except S3Error as err:
            print("Error:", err)
        
    np.save(os.path.join(log_folder, 'x_train.npy'), x_train)
    np.save(os.path.join(log_folder, 'y_train.npy'), y_train)
    np.save(os.path.join(log_folder, 'x_test.npy'), x_test)
    np.save(os.path.join(log_folder, 'y_test.npy'), y_test)
    result = NamedTuple('Outputs', [('x_train', str), ('y_train', str), ('x_test', str), ('y_test', str)])
    return result(
        os.path.join(log_folder, 'x_train.npy'),
        os.path.join(log_folder, 'y_train.npy'),
        os.path.join(log_folder, 'x_test.npy'),
        os.path.join(log_folder, 'y_test.npy')
    )
`;


const RANDOMFOREST =
`
def model_func(epochs:int, model_name:str, log_folder:str, x_train_path: str, y_train_path: str, x_test_path: str, y_test_path: str) -> NamedTuple('Outputs', [('logdir', str), ('accuracy', float)]):
    import tensorflow as tf
    import numpy as np
    import datetime
    import json
    import os
    from sklearn.metrics import accuracy_score
    
    import joblib
    print('model_func:', log_folder)
    
    x_train = np.load(x_train_path)
    y_train = np.load(y_train_path)
    x_test = np.load(x_test_path)
    y_test = np.load(y_test_path)
    
  

    def create_model():
        # START_MODEL_CODE
        from sklearn.ensemble import RandomForestClassifier
        return RandomForestClassifier(n_estimators=%s, criterion='%s', max_depth=%s, min_samples_split=%s, min_samples_leaf=%s)
        # END_MODEL_CODE
        
    model = create_model()
    
    ### add log
    log_dir = os.path.join(log_folder, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    ######
    
    # Train the model
    model.fit(x_train, y_train)
    
    # Get predictions
    y_pred = model.predict(x_test)
    
    # Get accuracy
    accuracy = accuracy_score(y_test, y_pred)
    
    model_path = os.path.join(log_folder, model_name, 'model.joblib') # remove 1
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    joblib.dump(model, model_path)
    
    print('logdir:', log_dir)
    print('accuracy', accuracy)
    accuracy = float(accuracy)
    return ([log_dir, accuracy])
def show_results(log_folder:str, accuracy: float) -> NamedTuple('Outputs', [('accuracy', float)]):
    import os

    accuracy_file_path = os.path.join(log_folder, 'accuracy')
    os.makedirs(os.path.dirname(accuracy_file_path), exist_ok=True)
    
    return ([accuracy])

@dsl.pipeline(
   name='Final pipeline',
   description='A pipeline to train a model on dataset output accuracy.'
)
`;


const LOGISTICREGRESSION =
`
def model_func(epochs:int, model_name:str, log_folder:str, x_train_path: str, y_train_path: str, x_test_path: str, y_test_path: str) -> NamedTuple('Outputs', [('logdir', str), ('accuracy', float)]):
    import tensorflow as tf
    import numpy as np
    import datetime
    import json
    import os
    from sklearn.metrics import accuracy_score 
    import joblib
    print('model_func:', log_folder)
    
    x_train = np.load(x_train_path)
    y_train = np.load(y_train_path)
    x_test = np.load(x_test_path)
    y_test = np.load(y_test_path)
    


    def create_model():
        # START_MODEL_CODE
        from sklearn.linear_model import LogisticRegression
        return LogisticRegression(penalty='%s', solver='%s' )
        # END_MODEL_CODE
        
        
    model = create_model()
    
    ### add log
    log_dir = os.path.join(log_folder, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    ######
    
    # Train the model
    model.fit(x_train, y_train)
    
    # Get predictions
    y_pred = model.predict(x_test)
    
    # Get accuracy
    accuracy = accuracy_score(y_test, y_pred)
    
    model_path = os.path.join(log_folder, model_name, 'model.joblib') # remove 1
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    joblib.dump(model, model_path)
    
    print('logdir:', log_dir)
    print('accuracy', accuracy)
    accuracy = float(accuracy)
    return ([log_dir, accuracy])
def show_results(log_folder:str, accuracy: float) -> NamedTuple('Outputs', [('accuracy', float)]):
    import os

    accuracy_file_path = os.path.join(log_folder, 'accuracy')
    os.makedirs(os.path.dirname(accuracy_file_path), exist_ok=True)
    
    return ([accuracy])

@dsl.pipeline(
   name='Final pipeline',
   description='A pipeline to train a model on dataset output accuracy.'
)
`;


const DECISIONTREE =
`
def model_func(epochs:int, model_name:str, log_folder:str, x_train_path: str, y_train_path: str, x_test_path: str, y_test_path: str) -> NamedTuple('Outputs', [('logdir', str), ('accuracy', float)]):
    import tensorflow as tf
    import numpy as np
    import datetime
    import json
    import os
    from sklearn.metrics import accuracy_score
    
    import joblib
    print('model_func:', log_folder)
    
    x_train = np.load(x_train_path)
    y_train = np.load(y_train_path)
    x_test = np.load(x_test_path)
    y_test = np.load(y_test_path)
    
   

    def create_model():
        # START_MODEL_CODE
        from sklearn.tree import DecisionTreeClassifier
        return DecisionTreeClassifier(criterion='%s', splitter='%s', max_depth=%s, min_samples_split=%s, min_samples_leaf=%s)
        # END_MODEL_CODE

        

        
    model = create_model()
    
    ### add log
    log_dir = os.path.join(log_folder, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    ######
    
    # Train the model
    model.fit(x_train, y_train)
    
    # Get predictions
    y_pred = model.predict(x_test)
    
    # Get accuracy
    accuracy = accuracy_score(y_test, y_pred)
    
    model_path = os.path.join(log_folder, model_name, 'model.joblib') # remove 1
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    joblib.dump(model, model_path)
    
    print('logdir:', log_dir)
    print('accuracy', accuracy)
    accuracy = float(accuracy)
    return ([log_dir, accuracy])
def show_results(log_folder:str, accuracy: float) -> NamedTuple('Outputs', [('accuracy', float)]):
    import os

    accuracy_file_path = os.path.join(log_folder, 'accuracy')
    os.makedirs(os.path.dirname(accuracy_file_path), exist_ok=True)
    
    return ([accuracy])
@dsl.pipeline(
   name='Final pipeline',
   description='A pipeline to train a model on dataset output accuracy.'
)
`;


const UPLOAD_RANDOMFOREST =
`
def randomforest_pipeline(epochs=%s, model_name="%s"):
# END_DEPLOY_CODE
    log_folder = '/data'
    pvc_name = "mypvc"
    vop = dsl.VolumeOp(
        name=pvc_name,
        resource_name="newpvc",
        size="1Gi",
        modes=dsl.VOLUME_MODE_RWO
    )
    
    load_data_op = func_to_container_op(
        func=load_data_func,
        base_image="tensorflow/tensorflow:2.14.0",
        packages_to_install=["pandas","minio"]
    )
    data_process_op = func_to_container_op(
        func=data_process_func,
        base_image="tensorflow/tensorflow:2.14.0",
        packages_to_install=["scikit-learn","pandas","minio"]
    )
    model_op = func_to_container_op(
        func=model_func,
        base_image="tensorflow/tensorflow:2.0.0-py3",
        packages_to_install=["scikit-learn"]
    )
    show_results_op = func_to_container_op(
        func=show_results,
        base_image="tensorflow/tensorflow:2.0.0-py3"
    )
    ########################################################
    load_data_task = load_data_op(log_folder).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")

    data_process_task = data_process_op(
        log_folder,
        load_data_task.outputs['samples'],
        load_data_task.outputs['labels'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")
    
    model_task = model_op(
        epochs,
        model_name,
        log_folder,
        data_process_task.outputs['x_train'],
        data_process_task.outputs['y_train'],
        data_process_task.outputs['x_test'],
        data_process_task.outputs['y_test'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")

    show_results_task = show_results_op(
        log_folder,
        model_task.outputs['accuracy'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")


pipeline_name='randomforest_pipeline{}.yaml'.format('%s')
kfp.compiler.Compiler().compile(randomforest_pipeline, pipeline_name)




import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = os.getenv("KUBEFLOW_HOST")
username = os.getenv("KUBEFLOW_USERNAME")
password = os.getenv("KUBEFLOW_PASSWORD")

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True

namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for only_randomforest."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile=pipeline_name
    name='pipeline-' + random_suffix()
    description="This is a only_randomforest pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'run_only_randomforest' + random_suffix()
    description = "This is a only_randomforest run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)

    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    


    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)
            #print(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions'] # Comfirm completion.
                    
            except KeyError:
                nodes = {}
                conditions = []

            output_value = None
            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False
            
            '''''
            def find_all_keys(node):
                if isinstance(node, dict):
                    for key in node.keys():
                        print("Key:", key)
                        find_all_keys(node[key])
                elif isinstance(node, list):
                    for item in node:
                        find_all_keys(item)

            # Call the function with your JSON data
            find_all_keys(output)
            '''''

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Pipeline is still running. Waiting...")
            time.sleep(polling_interval-1)
    
    found_final_pvc_name = False  # Add a variable to track if the PVC name has been found

    def find_final_pvc_name(node):
        global found_final_pvc_name  # Declare the variable as global

        if not found_final_pvc_name:  # If the PVC name has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'mypvc-name':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_final_pvc_name = True  # Set to True after finding the PVC name
                                print("mypvc-name:", value)
                                return value
                for key, value in node.items():
                    result = find_final_pvc_name(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_final_pvc_name(item)
                    if result:
                        return result

        return None
    
    find_final_pvc_name(output)  # Call the function to find final_pvc_name


    found_model_func_accuracy = False

    def find_model_func_accuracy(node):
        global found_model_func_accuracy  # Declare the variable as global

        if not found_model_func_accuracy:  # If the model-func-accuracy has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'model-func-accuracy':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_model_func_accuracy = True  # Set to True after finding model-func-accuracy
                                print("RANDOMFOREST Model Accuracy:", value)
                                return value

                for key, value in node.items():
                    result = find_model_func_accuracy(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_model_func_accuracy(item)
                    if result:
                        return result

        return None
    
    find_model_func_accuracy(output)
`;


const UPLOAD_DECISIONTREE =
`
def decisiontree_pipeline(epochs=%s, model_name="%s",):
# END_DEPLOY_CODE
    log_folder = '/data'
    pvc_name = "mypvc"
    vop = dsl.VolumeOp(
        name=pvc_name,
        resource_name="newpvc",
        size="1Gi",
        modes=dsl.VOLUME_MODE_RWO
    )
    
    load_data_op = func_to_container_op(
        func=load_data_func,
        base_image="tensorflow/tensorflow:2.14.0",
        packages_to_install=["minio","pandas"]
    )
    data_process_op = func_to_container_op(
        func=data_process_func,
        base_image="tensorflow/tensorflow:2.14.0",
        packages_to_install=["scikit-learn","pandas","minio"]
    )
    model_op = func_to_container_op(
        func=model_func,
        base_image="tensorflow/tensorflow:2.0.0-py3",
        packages_to_install=["scikit-learn"]
    )
    show_results_op = func_to_container_op(
        func=show_results,
        base_image="tensorflow/tensorflow:2.0.0-py3"
    )
    ########################################################
    load_data_task = load_data_op(log_folder).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")

    data_process_task = data_process_op(
        log_folder,
        load_data_task.outputs['samples'],
        load_data_task.outputs['labels'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")
    
    model_task = model_op(
        epochs,
        model_name,
        log_folder,
        data_process_task.outputs['x_train'],
        data_process_task.outputs['y_train'],
        data_process_task.outputs['x_test'],
        data_process_task.outputs['y_test'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")

    show_results_task = show_results_op(
        log_folder,
        model_task.outputs['accuracy'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")


pipeline_name='decisiontree_pipeline{}.yaml'.format('%s')
kfp.compiler.Compiler().compile(decisiontree_pipeline, pipeline_name)





import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = os.getenv("KUBEFLOW_HOST")
username = os.getenv("KUBEFLOW_USERNAME")
password = os.getenv("KUBEFLOW_PASSWORD")

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True

namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for only_decisiontree."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile=pipeline_name
    name='pipeline-' + random_suffix()
    description="This is a only_decisiontree pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'run_only_decisiontree' + random_suffix()
    description = "This is a only_decisiontree run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)

    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    


    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)
            #print(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions'] # Comfirm completion.
                    
            except KeyError:
                nodes = {}
                conditions = []

            output_value = None
            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False
            
            '''''
            def find_all_keys(node):
                if isinstance(node, dict):
                    for key in node.keys():
                        print("Key:", key)
                        find_all_keys(node[key])
                elif isinstance(node, list):
                    for item in node:
                        find_all_keys(item)

            # Call the function with your JSON data
            find_all_keys(output)
            '''''

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Pipeline is still running. Waiting...")
            time.sleep(polling_interval-1)
    
    found_final_pvc_name = False  # Add a variable to track if the PVC name has been found

    def find_final_pvc_name(node):
        global found_final_pvc_name  # Declare the variable as global

        if not found_final_pvc_name:  # If the PVC name has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'mypvc-name':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_final_pvc_name = True  # Set to True after finding the PVC name
                                print("mypvc-name:", value)
                                return value
                for key, value in node.items():
                    result = find_final_pvc_name(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_final_pvc_name(item)
                    if result:
                        return result

        return None
    
    find_final_pvc_name(output)  # Call the function to find final_pvc_name


    found_model_func_accuracy = False

    def find_model_func_accuracy(node):
        global found_model_func_accuracy  # Declare the variable as global

        if not found_model_func_accuracy:  # If the model-func-accuracy has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'model-func-accuracy':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_model_func_accuracy = True  # Set to True after finding model-func-accuracy
                                print("DECISIONTREE Model Accuracy:", value)
                                return value

                for key, value in node.items():
                    result = find_model_func_accuracy(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_model_func_accuracy(item)
                    if result:
                        return result

        return None
    
    find_model_func_accuracy(output)
`;


const UPLOAD_LOGISTICREGRESSION =
`
def logisticregression_pipeline(epochs=%s, model_name="%s",):

    log_folder = '/data'
    pvc_name = "mypvc"
    vop = dsl.VolumeOp(
        name=pvc_name,
        resource_name="newpvc",
        size="1Gi",
        modes=dsl.VOLUME_MODE_RWO
    )
    
    load_data_op = func_to_container_op(
        func=load_data_func,
        base_image="tensorflow/tensorflow:2.14.0",
        packages_to_install=["pandas","minio"],
    )
    data_process_op = func_to_container_op(
        func=data_process_func,
        base_image="tensorflow/tensorflow:2.14.0",
        packages_to_install=["scikit-learn","pandas","minio"]
    )
    model_op = func_to_container_op(
        func=model_func,
        base_image="tensorflow/tensorflow:2.0.0-py3",
        packages_to_install=["scikit-learn"]
    )
    show_results_op = func_to_container_op(
        func=show_results,
        base_image="tensorflow/tensorflow:2.0.0-py3"
    )
    ########################################################
    load_data_task = load_data_op(log_folder).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")

    data_process_task = data_process_op(
        log_folder,
        load_data_task.outputs['samples'],
        load_data_task.outputs['labels'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")
    
    model_task = model_op(
        epochs,
        model_name,
        log_folder,
        data_process_task.outputs['x_train'],
        data_process_task.outputs['y_train'],
        data_process_task.outputs['x_test'],
        data_process_task.outputs['y_test'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")

    show_results_task = show_results_op(
        log_folder,
        model_task.outputs['accuracy'],
    ).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")


pipeline_name='logisticregression_pipeline{}.yaml'.format('%s')    
kfp.compiler.Compiler().compile(logisticregression_pipeline,pipeline_name )




import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = os.getenv("KUBEFLOW_HOST")
username = os.getenv("KUBEFLOW_USERNAME")
password = os.getenv("KUBEFLOW_PASSWORD")

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True

namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for only_logisticregression."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile=pipeline_name
    name='pipeline-' + random_suffix()
    description="This is a only_logisticregression pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'run_only_logisticregression' + random_suffix()
    description = "This is a only_logisticregression run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)

    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    


    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)
            #print(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions'] # Comfirm completion.
                    
            except KeyError:
                nodes = {}
                conditions = []

            output_value = None
            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False
            
            '''''
            def find_all_keys(node):
                if isinstance(node, dict):
                    for key in node.keys():
                        print("Key:", key)
                        find_all_keys(node[key])
                elif isinstance(node, list):
                    for item in node:
                        find_all_keys(item)

            # Call the function with your JSON data
            find_all_keys(output)
            '''''

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Pipeline is still running. Waiting...")
            time.sleep(polling_interval-1)
    
    found_final_pvc_name = False  # Add a variable to track if the PVC name has been found

    def find_final_pvc_name(node):
        global found_final_pvc_name  # Declare the variable as global

        if not found_final_pvc_name:  # If the PVC name has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'mypvc-name':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_final_pvc_name = True  # Set to True after finding the PVC name
                                print("mypvc-name:", value)
                                return value
                for key, value in node.items():
                    result = find_final_pvc_name(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_final_pvc_name(item)
                    if result:
                        return result

        return None
    
    find_final_pvc_name(output)  # Call the function to find final_pvc_name


    found_model_func_accuracy = False

    def find_model_func_accuracy(node):
        global found_model_func_accuracy  # Declare the variable as global

        if not found_model_func_accuracy:  # If the model-func-accuracy has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'model-func-accuracy':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_model_func_accuracy = True  # Set to True after finding model-func-accuracy
                                print("LOGISTICREGRESSION Model Accuracy:", value)
                                return value

                for key, value in node.items():
                    result = find_model_func_accuracy(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_model_func_accuracy(item)
                    if result:
                        return result

        return None
    
    find_model_func_accuracy(output)
`;


const DEPLOY_KSERVE =
`
from __future__ import print_function

import re
import kfp
import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp import components
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = os.getenv("KUBEFLOW_HOST")
username = os.getenv("KUBEFLOW_USERNAME")
password = os.getenv("KUBEFLOW_PASSWORD")

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True
namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

namespaces = str(namespaces)
namespaces = namespaces[2:-2]
#print(namespaces)


kfserving_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2.0.2/components/kserve/component.yaml')
@dsl.pipeline(
  name='KFServing pipeline',
  description='A pipeline for KFServing with PVC.'
)
def kfservingPipeline(
    action='apply',
    namespace=namespaces,
    
    pvc_name='%s', # change pvc_name
    model_name='%s'): # change model_name

    # specify the model dir located on pvc
    model_pvc_uri = 'pvc://{}/{}/'.format(pvc_name, model_name)
    
    # create inference service resource named by model_name
    isvc_yaml = '''
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: {}
  namespace: {}
spec:
  predictor:
    sklearn:
      storageUri: {}
      resources:
        limits:
          cpu: "100m"
        requests:
          cpu: "100m"
'''.format(model_name, namespace, model_pvc_uri)
    
    
    
    kfserving = kfserving_op(
        action=action,
        inferenceservice_yaml=isvc_yaml,
    )
    kfserving.set_cpu_request("500m").set_cpu_limit("500m")

    return([isvc_yaml])


# Compile pipeline
pipeline_name = 'sklearn-kserve{}.yaml'.format("%s")
kfp.compiler.Compiler().compile(kfservingPipeline, pipeline_name)


host = os.getenv("KUBEFLOW_HOST")
username = os.getenv("KUBEFLOW_USERNAME")
password = os.getenv("KUBEFLOW_PASSWORD")

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True
namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for only_kfserving."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile=pipeline_name
    name='pipeline-' + random_suffix()
    description="This is a only_kfserving pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'run_only_kfserving' + random_suffix()
    description = "This is a only_kfserving run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)
    
    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions']  # Confirm completion.

            except KeyError:
                nodes = {}
                conditions = []

            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Pipeline is still running. Waiting...")
            time.sleep(polling_interval-1)
     
    found_status = False  # Add a variable to track if the status has been found

    def find_status(node):
        global found_status  # Declare the variable as a global variable

        if not found_status:  # If the status has not been found yet
            if isinstance(node, dict):
                if 'status' in node:
                    status = node['status']
                    if 'phase' in status:
                        if status['phase'] == 'Failed':
                            print("status: Failed Deployment!")
                        elif status['phase'] == 'Succeeded':
                            print("status: Successful Deployment!")
                    for key, value in node.items():
                        result = find_status(value)
                        if result:
                            return result
            elif isinstance(node, list):
                for item in node:
                    result = find_status(item)
                    if result:
                        return result

        return None

    # Call the function to find and print the status
    find_status(output)
`;


const MODELPREDICT =
`
from typing import NamedTuple
import kfp
from kfp import dsl
from kfp.components import func_to_container_op, InputPath, OutputPath
import kfp.components as components
import datetime
import os

def predict_func(log_dir:str):
    import json
    import requests
    import cv2
    import pandas as pd
    import string
    import time
    import kfp_server_api
    import os
    import kfp
    import numpy as np
    from minio import Minio
    from io import StringIO, BytesIO 
    from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
    import datetime
    import pickle
    import re
    from urllib.parse import urlsplit  
    
    def get_istio_auth_session(url: str, username: str, password: str) -> dict:
    
        # define the default return object
        auth_session = {
            "endpoint_url": url,    # KF endpoint URL
            "redirect_url": None,   # KF redirect URL, if applicable
            "dex_login_url": None,  # Dex login URL (for POST of credentials)
            "is_secured": None,     # True if KF endpoint is secured
            "session_cookie": None  # Resulting session cookies in the form "key1=value1; key2=value2"
        }

        # use a persistent session (for cookies)
        with requests.Session() as s:

            ################
            # Determine if Endpoint is Secured
            ################
            resp = s.get(url, allow_redirects=True)
            if resp.status_code != 200:
                raise RuntimeError(
                    f"HTTP status code '{resp.status_code}' for GET against: {url}"
                )

            auth_session["redirect_url"] = resp.url

            # if we were NOT redirected, then the endpoint is UNSECURED
            if len(resp.history) == 0:
                auth_session["is_secured"] = False
                return auth_session
            else:
                auth_session["is_secured"] = True

            ################
            # Get Dex Login URL
            ################
            redirect_url_obj = urlsplit(auth_session["redirect_url"])

            
            if re.search(r"/auth$", redirect_url_obj.path):
                # default to "staticPasswords" auth type
                redirect_url_obj = redirect_url_obj._replace(
                    path=re.sub(r"/auth$", "/auth/local", redirect_url_obj.path)
                )

            
            if re.search(r"/auth/.*/login$", redirect_url_obj.path):
                auth_session["dex_login_url"] = redirect_url_obj.geturl()

            
            else:
                resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)
                if resp.status_code != 200:
                    raise RuntimeError(
                        f"HTTP status code '{resp.status_code}' for GET against: {redirect_url_obj.geturl()}"
                    )

                # set the login url
                auth_session["dex_login_url"] = resp.url

            ################
            # Attempt Dex Login
            ################
            resp = s.post(
                auth_session["dex_login_url"],
                data={"login": username, "password": password},
                allow_redirects=True
            )
            if len(resp.history) == 0:
                raise RuntimeError(
                    f"Login credentials were probably invalid - "
                    f"No redirect after POST to: {auth_session['dex_login_url']}"
                )

            # store the session cookies in a "key1=value1; key2=value2" string
            auth_session["session_cookie"] = "; ".join([f"{c.name}={c.value}" for c in s.cookies])

        return auth_session

    host = "%s"
    username = "%s"
    password = "%s"

    auth_session = get_istio_auth_session(
            url=host,
            username=username,
            password=password
        )

    configuration = kfp_server_api.Configuration(
        host = os.path.join(host, "pipeline"),
    )
    configuration.debug = True

    model_name = '%s' 
    auth = 'authservice_session={}'.format(auth_session['session_cookie'][20:])
    host = '{}-predictor-default.kubeflow-user-thu10.ai4edu.thu01.footprint-ai.com'.format(model_name)
    predict_url = 'https://ai4edu.thu01.footprint-ai.com/v1/models/{}:predict'.format(model_name)

    min_host = "%s"
    min_username = "%s"
    min_password = "%s"

        
    # Initialize MinIO client
    client = Minio(min_host ,
                access_key=min_username,
                secret_key=min_password ,
                secure=False)

    bucket_name = "sample-bucket"
    object_name = "%s"

    
    response = client.get_object(bucket_name, object_name)
    

    if "%s" != 'image':
        try:
            csvFile = response.read().decode('utf-8')
        except UnicodeDecodeError as ude:
            raise ValueError("Failed to decode the data from MinIO") from ude
            
        data = pd.read_csv(StringIO(csvFile))
        col_names = data.columns.tolist()
        data = data.values
        
    else:
        image_data = response.read()
        numpy_array = np.frombuffer(image_data, np.uint8)
        orig = cv2.imdecode(numpy_array, cv2.%s)
        data = cv2.resize(orig, (%s,%s), interpolation = cv2.INTER_AREA)
      

        
    client.remove_object(bucket_name, object_name)

    scaler = False
    if %s:
        response = client.get_object("scalers", 'scaler.pkl')
        scaler_data = response.read()
        scaler = pickle.loads(scaler_data)

        client.remove_object("scalers", "scaler.pkl")
    


    %s
    %s
    %s
    if scaler:
        data = np.asarray([data.tolist()])
        data = data.reshape(data.shape[0],-1)
        data = scaler.transform(data)  
    %s    


    headers = {'Host': host, 'Cookie': auth}
    classnames = %s
    payload={"signature_name": "serving_default", "instances": data.tolist()}
    resp = requests.post(predict_url, headers=headers, data=json.dumps(payload))
    resp_json = json.loads(resp.content)
    pred = []
    for p in resp_json['predictions']:
        pred.append('prediction: {}'.format(classnames[p]))

    pred_str = "\\n".join(pred)
    result_object_name = "prediction.txt"
    result_bytes = pred_str.encode('utf-8')



    client.put_object(
        bucket_name=bucket_name,
        object_name=result_object_name,
        data=BytesIO(result_bytes),
        length=len(result_bytes),
        content_type='text/plain'
    )

    return(str(pred))
@dsl.pipeline(
   name='Predict pipeline',
   description='A pipeline to predict data on model.'
)


def predict_pipeline():
    log_folder = '/data'
    pvc_name = "predpvc"
    vop = dsl.VolumeOp(
        name=pvc_name,
        resource_name="ppvc",
        size="1Gi",
        modes=dsl.VOLUME_MODE_RWO
    )
    
    pred_op = func_to_container_op(
        func=predict_func,
        base_image="python:3.8-slim",
        packages_to_install=["minio<7.0.0", "pandas", "scikit-learn", "opencv-python-headless==4.5.3.56", "kubernetes>=12.0.0", "kfp==1.8.9", "requests==2.31.0", "urllib3==1.26.0"]
    )
    
    pred_task = pred_op(log_folder).add_pvolumes({
        log_folder:vop.volume,
    }).set_cpu_limit("1").set_cpu_request("0.5")

    


kfp.compiler.Compiler().compile(predict_pipeline, "pred.yaml")


import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = os.getenv("KUBEFLOW_HOST")
username = os.getenv("KUBEFLOW_USERNAME")
password = os.getenv("KUBEFLOW_PASSWORD")

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True

namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for model predict."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile="pred.yaml"
    name='pipeline-' + random_suffix()
    description="This is a model predict pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'run_model_predict' + random_suffix()
    description = "This is a model predict run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)

    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    


    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)
            #print(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions'] # Comfirm completion.
                    
            except KeyError:
                nodes = {}
                conditions = []

            output_value = None
            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False
            
            '''''
            def find_all_keys(node):
                if isinstance(node, dict):
                    for key in node.keys():
                        print("Key:", key)
                        find_all_keys(node[key])
                elif isinstance(node, list):
                    for item in node:
                        find_all_keys(item)

            # Call the function with your JSON data
            find_all_keys(output)
            '''''

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Predicting ...")
            time.sleep(polling_interval-1)
`

const RETRIEVEPRED =
`
from minio import Minio
from minio.error import S3Error
from io import BytesIO
import os

# MinIO配置
host = os.getenv("MINIO_HOST")
username = os.getenv("MINIO_ROOT_USER")
password = os.getenv("MINIO_ROOT_PASSWORD")

# 初始化MinIO客戶端
client = Minio(host,
               access_key=username,
               secret_key=password,
               secure=False)

bucket_name = "sample-bucket"

try:
    response = client.get_object(bucket_name, "prediction.txt")
    predictions_from_minio = response.read().decode('utf-8')
except S3Error as err:
    print("Error:", err)
else:
    print(predictions_from_minio)

    
    try:
        client.remove_object(bucket_name, "prediction.txt")
    except S3Error as err:
        print("Error while deleting the object:", err)

`


module.exports = {
    IMPORTS,
    DATA_UPLOAD,
    P_DATA_UPLOAD,
    DATASETS,
    DATASETS_CUSTOMIZED,
    DATAPROCESS,
    FILLNANBYMEAN,
    P_FILLNANBYMEDIAN,
    FILLNANBYMEDIAN,
    P_FILLNANBYMEAN,
    FILLNANBYVALUE,
    P_FILLNANBYVALUE,
    LABELENCODING,
    P_LABELENCODING,
    ONEHOTNCODING,
    P_ONEHOTNCODING,
    STANDARDLIZATION,
    NORMALIZATION,
    DATA_RESHAPE1D,
    P_DATA_RESHAPE1D,
    DATA_RESHAPE2D,
    P_DATA_RESHAPE2D,
    RANDOMFOREST,
    DECISIONTREE,
    LOGISTICREGRESSION,
    UPLOAD_RANDOMFOREST,
    UPLOAD_DECISIONTREE,
    UPLOAD_LOGISTICREGRESSION,
    DEPLOY_KSERVE,
    MODELPREDICT,
    RETRIEVEPRED,
    FL,
};