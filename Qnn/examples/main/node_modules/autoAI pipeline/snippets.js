'use strict';

const IMPORTS =
`
from typing import NamedTuple
import kfp
from kfp import dsl
from kfp.components import func_to_container_op, InputPath, OutputPath
import kfp.components as components
import datetime
import os
`;


const DATA_UPLOAD =
`
from minio import Minio
from minio.error import S3Error
import os 
import csv

host = os.getenv("MINIO_HOST")
username = os.getenv("MINIO_ROOT_USER")
password = os.getenv("MINIO_ROOT_PASSWORD")

      
# Initialize MinIO client
client = Minio(host ,
               access_key=username,
               secret_key=password,
               secure=False)


local_file = "%s"
bucket_name = "data-bucket"
object_name = "%s"
label = %s
unique_labels = set()

with open(local_file, 'r') as f:
    reader = csv.reader(f)
    column_names = next(reader)  
    label_idx = column_names.index(label)
    print("COLUMN_NAMES:", str(column_names))
    for row in reader:
        unique_labels.add(row[label_idx])
    print("Classnames:", str([str(i) for i in unique_labels]))


if not client.bucket_exists(bucket_name):
    client.make_bucket(bucket_name)
try:
    result = client.fput_object(bucket_name, object_name, local_file)
    print("Uploaded", result.object_name, "with ETag:", result.etag)
except S3Error as err:
    print("Error:", err)

`;


const DATASETS_CUSTOMIZED =
`
def load_data_func(log_folder:str) -> NamedTuple('Outputs', [('samples', str), ('labels', str)]):
    from typing import NamedTuple
    import os
    import numpy as np
    import pandas as pd
    from io import StringIO
    from minio import Minio
    from minio.error import S3Error

    host = "http://%s"
    username = "%s"
    password = "%s"

        
    # Initialize MinIO client
    client = Minio(host ,
                access_key=username,
                secret_key=password ,
                secure=False)

    bucket_name = "data-bucket"
    object_name = "%s"


    try:
        response = client.get_object(bucket_name, object_name)
        
        try:
            dataset = response.read().decode('utf-8')
        except UnicodeDecodeError as ude:
            raise ValueError("Failed to decode the data from MinIO") from ude
                
        df = pd.read_csv(StringIO(dataset))

        client.remove_object(bucket_name, object_name)
    except S3Error as err:
        print("Error:", err)
        

    label_column = %s 
    data = df.drop(label_column, axis=1).values
    target = df[label_column].values
    
    
    np.save(os.path.join(log_folder, 'samples.npy'), data)
    np.save(os.path.join(log_folder, 'labels.npy'), target)
    result = NamedTuple('Outputs', [('samples', str), ('labels', str)])
    return result(
        os.path.join(log_folder, 'samples.npy'),
        os.path.join(log_folder, 'labels.npy')
    )
`;


const DATASETS =
`
def download_data(log_folder:str) -> NamedTuple('Outputs', [('x_train', str), ('y_train', str), ('x_test', str), ('y_test', str)]):
    import json
    import numpy as np
    import tensorflow as tf
    from tensorflow import keras
    import os
    from typing import NamedTuple
    dataset = tf.keras.datasets.%s
    # END_DATASET_CODE

    (x_train, y_train), (x_test, y_test) = dataset.load_data()
    
    idx_train = np.append(
    np.where(y_train == 0)[0][:100], np.where(y_train == 1)[0][:100]
    )
    x_train = x_train[idx_train]
    y_train = y_train[idx_train]


    idx_test = np.append(
        np.where(y_test == 0)[0][:50], np.where(y_test == 1)[0][:50]
    )

    x_test = x_test[idx_test]
    y_test = y_test[idx_test]
    
    np.save(os.path.join(log_folder, 'x_train.npy'), x_train)
    np.save(os.path.join(log_folder, 'y_train.npy'), y_train)
    np.save(os.path.join(log_folder, 'x_test.npy'), x_test)
    np.save(os.path.join(log_folder, 'y_test.npy'), y_test)
    result = NamedTuple('Outputs', [('x_train', str), ('y_train', str), ('x_test', str), ('y_test', str)])
    return result(
        os.path.join(log_folder, 'x_train.npy'),
        os.path.join(log_folder, 'y_train.npy'),
        os.path.join(log_folder, 'x_test.npy'),
        os.path.join(log_folder, 'y_test.npy')
    )
`;


const FILLNANBYVALUE =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    data = pd.DataFrame(samples, columns=col_names[:])  
    data.fillna(value=%s, inplace=True)  
    samples = data.values
`   

const FILLNANBYMEAN =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    data = pd.DataFrame(samples, columns=col_names[:])  
    data.fillna(value=df.mean(), inplace=True)  # 使用每列的平均值填充該列的NaN值 
    samples = data.values
`

const FILLNANBYMEDIAN =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    data = pd.DataFrame(samples, columns=col_names[:])  
    data.fillna(value=df.median(), inplace=True)  # 使用每列的中位數填充該列的NaN值
    samples = data.values
`

const LABELENCODING =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    process_column = %s
    encoder = LabelEncoder()
    data = pd.DataFrame(samples, columns=col_names[:])  
    target = pd.DataFrame(labels, columns=[label_column_name])
    for column in process_column:
        if column == label_column_name:
            target[column] = encoder.fit_transform(target[column])
        else:
            data[column] = encoder.fit_transform(data[column])

    samples = data.values
    labels = target.values
`


const ONEHOTNCODING =
`
    label_column_name = %s
    col_names = %s
    col_names.remove(label_col_name)
    process_column = %s
    data = pd.DataFrame(samples, columns=col_names[:])  
    target = pd.DataFrame(labels, columns=[label_column_name])
    for column in process_column:
        if column == label_column_name:
            target = pd.get_dummies(target, columns=[column])
        else:
            data = pd.get_dummies(data, columns=[column])

    samples = data.values
    labels = target.values
`

const NORMALIZATION =
`   
    train_data = train_data.reshape(train_data.shape[0],-1)
    test_data = test_data.reshape(test_data.shape[0],-1)
    train_data = train_data/255
    test_data = test_data/255
`


const STANDARDLIZATION =
`   
    train_data = train_data.reshape(train_data.shape[0],-1)
    test_data = test_data.reshape(test_data.shape[0],-1)
    scaler = StandardScaler()
    train_data = scaler.fit_transform(train_data)
    test_data = scaler.transform(test_data)
`


const DATA_RESHAPE1D =
`
    train_data = torch.tensor(train_data, dtype=torch.float32)
    train_label = torch.tensor(np.resize(train_label,(train_label.shape[0],1)))
    test_data = torch.tensor(test_data, dtype=torch.float32)
    test_label = torch.tensor(np.resize(test_label,(test_label.shape[0],1)))
`


const DATA_RESHAPE2D =
`
    train_data = torch.tensor(np.resize(train_data,(train_data.shape[0],1,%s,%s,%s)), dtype=torch.float32)
    train_label = torch.tensor(np.resize(train_label,(train_label.shape[0],1)))
    test_data = torch.tensor(np.resize(test_data,(test_data.shape[0],1,%s,%s,%s)), dtype=torch.float32)
    test_label = torch.tensor(np.resize(test_label,(test_label.shape[0],1)))
`


const DATAPROCESS =
`
def data_process_func(log_folder:str, 
        x_train_path: str, 
        y_train_path: str, 
        x_test_path: str, 
        y_test_path: str) -> NamedTuple('Outputs', [('x_train', str), ('y_train', str), ('x_test', str), ('y_test', str)]):
    from typing import NamedTuple
    import os
    import torch
    import numpy as np
    import pandas as pd
    from sklearn.preprocessing import LabelEncoder
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.preprocessing import StandardScaler
    import pickle


    train_data=np.load(x_train_path)
    train_label=np.load(y_train_path)
    test_data=np.load(x_test_path)
    test_label=np.load(y_test_path)


    %s
    %s
    %s
    %s
    %s



    with open(os.path.join(log_folder, 'train_data'), 'wb') as data_file:
        pickle.dump(train_data, data_file)

    # 
    with open(os.path.join(log_folder, 'train_label'), 'wb') as label_file:
        pickle.dump(train_label, label_file)

    # 
    with open(os.path.join(log_folder, 'test_data'), 'wb') as data_file:
        pickle.dump(test_data, data_file)

    # 
    with open(os.path.join(log_folder, 'test_label'), 'wb') as label_file:
        pickle.dump(test_label, label_file)
    result = NamedTuple('Outputs', [('x_train', str), ('y_train', str), ('x_test', str), ('y_test', str)])    
    return result(
    os.path.join(log_folder, 'train_data'),
    os.path.join(log_folder, 'train_label'),
    os.path.join(log_folder, 'test_data'),
    os.path.join(log_folder, 'test_label')
    )
`

const ZZ_MAP = 
`
        feature_map = ZZFeatureMap(feature_dimension=%s)
`;

const PAULI_MAP =
`
        feature_map = PauliFeatureMap(feature_dimension=%s, paulis=['X', 'Y', 'ZZ'])
`;

const RA =
`
        ansatz = RealAmplitudes(num_qubits=%s, reps=%s, entanglement="%s")
`;

const EF = 
`
        ansatz = EfficientSU2(num_qubits=%s, reps=%s, entanglement="%s")
`
const QNN =
`
def qnn_test(log_folder:str,
            epochs:int, 
            model_name:str,
            x_train_path: str, 
            y_train_path: str, 
            x_test_path: str, 
            y_test_path: str)-> NamedTuple('Outputs', [('logdir', str),('test_acc', float)]):
    import json
    import torch
    import numpy as np
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes, PauliFeatureMap, EfficientSU2
    from qiskit import QuantumCircuit
    from qiskit_machine_learning.neural_networks import EstimatorQNN
    from qiskit_machine_learning.connectors import TorchConnector
    import os
    import joblib
    import datetime
    # Define and create QNN
    def create_qnn():
        %s
        %s
        qc = QuantumCircuit(%s)
        qc.compose(feature_map, inplace=True)
        qc.compose(ansatz, inplace=True)

        qnn = EstimatorQNN(
            circuit=qc,
            input_params=feature_map.parameters,
            weight_params=ansatz.parameters,
            input_gradients=True,
        )
        return qnn

    class Net(nn.Module):
        def __init__(self, qnn):
            super().__init__()
            self.conv1 = nn.Conv2d(1, 2, kernel_size=5)
            self.conv2 = nn.Conv2d(2, 16, kernel_size=5)
            self.dropout = nn.Dropout2d()
            self.fc1 = nn.Linear(256, 64)
            self.fc2 = nn.Linear(64, 2)
            self.qnn = TorchConnector(qnn)
            self.fc3 = nn.Linear(1, 1)

        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.max_pool2d(x, 2)
            x = F.relu(self.conv2(x))
            x = F.max_pool2d(x, 2)
            x = self.dropout(x)
            x = x.view(x.shape[0], -1)
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            x = self.qnn(x)
            x = self.fc3(x)
            return torch.cat((x, 1 - x), -1)



    
    import pickle


    with open(x_train_path, 'rb') as data_file:
        x_train = pickle.load(data_file)


    with open(y_train_path, 'rb') as label_file:
        y_train = pickle.load(label_file)


    with open(x_test_path, 'rb') as data_file:
        x_test = pickle.load(data_file)


    with open(y_test_path, 'rb') as label_file:
        y_test = pickle.load(label_file)


    log_dir = os.path.join(log_folder, datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    train=[x_train,y_train] 
    test=[x_test,y_test]
    # Define model, optimizer, and loss function
    qnn = create_qnn()
    model = Net(qnn)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    loss_func = nn.NLLLoss()

    # Set up data loaders (you need to define train_loader and test_loader)

    # Start training
    epochs = epochs  # Set number of epochs
    loss_list = []  # Store loss history
    model.train()  # Set model to training mode
    batch_size=1

    for epoch in range(epochs):
        total_loss = []
        train_acc = 0
        for i in range(len(train[0])):
            optimizer.zero_grad(set_to_none=True)  # Initialize gradient
            output = model(train[0][i])  # Forward pass

            loss = loss_func(output, train[1][i])  # Calculate loss
            loss.backward()  # Backward pass
            optimizer.step()  # Optimize weights
            total_loss.append(loss.item())  # Store loss
            pred = output.argmax(dim=1, keepdim=True)
            train_acc += pred.eq(train[1][i].view_as(pred)).sum().item()

        loss_list.append(sum(total_loss) / len(total_loss))
    train_acc = (train_acc / len(train[0])) /  batch_size * 100
    model.eval()
    with torch.no_grad():
        test_acc = 0
        total_loss = []
        for i in range(len(test[0])):
            output = model(test[0][i])
            if len(output.shape) == 1:
                output = output.reshape(1, *output.shape)
            pred = output.argmax(dim=1, keepdim=True)
            test_acc += pred.eq(test[1][i].view_as(pred)).sum().item()
            loss = loss_func(output, test[1][i])
            total_loss.append(loss.item())

    test_acc = test_acc / len(test[0]) /  batch_size * 100
    average_loss = sum(total_loss) / len(total_loss)
    



    model_path = os.path.join(log_folder, model_name, 'model.joblib')
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    torch.save(model.state_dict(), model_path)

    print('logdir:', log_dir)
    print('accuracy', test_acc)

    return ([log_dir, test_acc])



def show_results(log_folder:str, test_acc: float) -> NamedTuple('Outputs', [('test_accuracy', float)]):
    import os
    test_accuracy_file_path = os.path.join(log_folder, 'test_accuracy')
    os.makedirs(os.path.dirname(test_accuracy_file_path), exist_ok=True)

    return ([test_acc])

@dsl.pipeline(
    name='QNN test'
)

`;


const UPLOAD_QNN=
`
def QNN_pipeline(epochs=%s, model_name="%s"):
    log_folder = '/data'
    pvc_name = "mypvc"
    vop = dsl.VolumeOp(
        name=pvc_name,
        resource_name="newpvc",
        size="1Gi",
        modes=dsl.VOLUME_MODE_RWO
    )
    download_op = func_to_container_op(download_data,base_image='tensorflow/tensorflow')
    data_process_op = func_to_container_op(data_process_func,base_image='pytorch/pytorch',packages_to_install=['pandas','scikit-learn'])
    qnn_op = func_to_container_op(qnn_test,base_image='pytorch/pytorch',packages_to_install=['qiskit','qiskit-machine-learning'])
    show_results_op = func_to_container_op(show_results)
    
    download_task = download_op(log_folder).set_cpu_request('0.2').set_cpu_limit('0.2').add_pvolumes({
        log_folder:vop.volume,
    })
    data_process_task = data_process_op(log_folder,
                                         download_task.outputs['x_train'],
                                        download_task.outputs['y_train'],
                                        download_task.outputs['x_test'],
                                       download_task.outputs['y_test']).set_cpu_request('0.5').set_cpu_limit('0.5').add_pvolumes({
                                        log_folder:vop.volume,
                                        })
    
    qnn_task = qnn_op(log_folder,
                      epochs,
                      model_name,
                      data_process_task.outputs['x_train'],
                      data_process_task.outputs['y_train'],
                      data_process_task.outputs['x_test'],
                      data_process_task.outputs['y_test']
                      ).set_cpu_request('0.5').set_cpu_limit('0.5').add_pvolumes({
                                        log_folder:vop.volume,
                                        })
    
    show_results_task = show_results_op(log_folder,
                                       qnn_task.outputs['test_acc']
                                       ).set_cpu_request('0.5').set_cpu_limit('0.5').add_pvolumes({
                                        log_folder:vop.volume,
                                        })
    

kfp.compiler.Compiler().compile(QNN_pipeline, 'qnn_pipeline.yaml')

import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = "http://ai4edu.thu01.footprint-ai.com"
username = "thu10"
password = "2[=tYABz"

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True

namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for only_qnn."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile='qnn_pipeline.yaml'
    name='pipeline-' + random_suffix()
    description="This is a only_qnn pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'run_only_qnn' + random_suffix()
    description = "This is a only_qnn run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)

    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    


    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)
            #print(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions'] # Comfirm completion.
                    
            except KeyError:
                nodes = {}
                conditions = []

            output_value = None
            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False
            
            '''''
            def find_all_keys(node):
                if isinstance(node, dict):
                    for key in node.keys():
                        print("Key:", key)
                        find_all_keys(node[key])
                elif isinstance(node, list):
                    for item in node:
                        find_all_keys(item)

            # Call the function with your JSON data
            find_all_keys(output)
            '''''

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Pipeline is still running. Waiting...")
            time.sleep(polling_interval-1)
    
    found_qnn_pvc_name = False  # Add a variable to track if the PVC name has been found

    def find_qnn_pvc_name(node):
        global found_qnn_pvc_name  # Declare the variable as global

        if not found_qnn_pvc_name:  # If the PVC name has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'mypvc-name':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_qnn_pvc_name = True  # Set to True after finding the PVC name
                                print("mypvc-name:", value)
                                return value
                for key, value in node.items():
                    result = find_qnn_pvc_name(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_qnn_pvc_name(item)
                    if result:
                        return result

        return None
    
    find_qnn_pvc_name(output)  # Call the function to find final_pvc_name


    found_qnn_test_accuracy = False

    def find_qnn_test_accuracy(node):
        global found_qnn_test_accuracy  # Declare the variable as global

        if not found_qnn_test_accuracy:  # If the model-func-accuracy has not been found yet
            if isinstance(node, dict):
                if 'parameters' in node:
                    parameters = node['parameters']
                    for parameter in parameters:
                        if 'name' in parameter and parameter['name'] == 'qnn-test-test_acc':
                            value = parameter.get('value')
                            if value and not value.startswith('{{') and not value.endswith('}}'):
                                found_qnn_test_accuracy = True  # Set to True after finding model-func-accuracy
                                print("qnn-test-accuracy:", value)
                                return value

                for key, value in node.items():
                    result = find_qnn_test_accuracy(value)
                    if result:
                        return result
            elif isinstance(node, list):
                for item in node:
                    result = find_qnn_test_accuracy(item)
                    if result:
                        return result

        return None
    
    find_qnn_test_accuracy(output)
`;


const DEPLOY_KSERVE =
`
from __future__ import print_function

import re
import kfp
import time
import kfp_server_api
import os
import requests
import string
import random
import json
from kfp import dsl
from kfp import components
from kfp.components import func_to_container_op, OutputPath
from kfp_server_api.rest import ApiException
from pprint import pprint
from kfp_login import get_istio_auth_session
from kfp_namespace import retrieve_namespaces

host = os.getenv("KUBEFLOW_HOST")
username = os.getenv("KUBEFLOW_USERNAME")
password = os.getenv("KUBEFLOW_PASSWORD")

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True
namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

namespaces = str(namespaces)
namespaces = namespaces[2:-2]
#print(namespaces)


kfserving_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2.0.2/components/kserve/component.yaml')
@dsl.pipeline(
  name='KFServing pipeline',
  description='A pipeline for KFServing with PVC.'
)
def kfservingPipeline(
    action='apply',
    namespace=namespaces,
    
    pvc_name='%s', # change pvc_name
    model_name='%s'): # change model_name

    # specify the model dir located on pvc
    model_pvc_uri = 'pvc://{}/{}/'.format(pvc_name, model_name)
    
    # create inference service resource named by model_name
    isvc_yaml = '''
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: {}
  namespace: {}
spec:
  predictor:
    sklearn:
      storageUri: {}
      resources:
        limits:
          cpu: "300m"
        requests:
          cpu: "300m"
'''.format(model_name, namespace, model_pvc_uri)
    
    
    
    kfserving = kfserving_op(
        action=action,
        inferenceservice_yaml=isvc_yaml,
    )
    kfserving.set_cpu_request("500m").set_cpu_limit("500m")

    return([isvc_yaml])


# Compile pipeline
kfp.compiler.Compiler().compile(kfservingPipeline, 'sklearn-kserve.yaml')


host = os.getenv("KUBEFLOW_HOST")
username = os.getenv("KUBEFLOW_USERNAME")
password = os.getenv("KUBEFLOW_PASSWORD")

auth_session = get_istio_auth_session(
        url=host,
        username=username,
        password=password
    )

# The client must configure the authentication and authorization parameters
# in accordance with the API server security policy.
# Examples for each auth method are provided below, use the example that
# satisfies your auth use case.

# Configure API key authorization: Bearer
configuration = kfp_server_api.Configuration(
    host = os.path.join(host, "pipeline"),
)
configuration.debug = True
namespaces = retrieve_namespaces(host, auth_session)
#print("available namespace: {}".format(namespaces))

def random_suffix() :
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))

# Enter a context with an instance of the API client
with kfp_server_api.ApiClient(configuration, cookie=auth_session["session_cookie"]) as api_client:
    # Create an instance of the  Experiment API class
    experiment_api_instance = kfp_server_api.ExperimentServiceApi(api_client)
    name="experiment-" + random_suffix()
    description="This is a experiment for only_kfserving."
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
        key=kfp_server_api.models.ApiResourceKey(
            type=kfp_server_api.models.ApiResourceType.NAMESPACE,
            id=resource_reference_key_id
        ),
        relationship=kfp_server_api.models.ApiRelationship.OWNER
    )]
    body = kfp_server_api.ApiExperiment(name=name, description=description, resource_references=resource_references) # ApiExperiment | The experiment to be created.
    try:
        # Creates a new experiment.
        experiment_api_response = experiment_api_instance.create_experiment(body)
        experiment_id = experiment_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling ExperimentServiceApi->create_experiment: %s\\n" % e)
    
    # Create an instance of the pipeline API class
    api_instance = kfp_server_api.PipelineUploadServiceApi(api_client) 
    uploadfile='sklearn-kserve.yaml'
    name='pipeline-' + random_suffix()
    description="This is a only_kfserving pipline."
    try:
        pipeline_api_response = api_instance.upload_pipeline(uploadfile, name=name, description=description)
        pipeline_id = pipeline_api_response.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling PipelineUploadServiceApi->upload_pipeline: %s\\n" % e)

    # Create an instance of the run API class
    run_api_instance = kfp_server_api.RunServiceApi(api_client)
    display_name = 'run_only_kfserving' + random_suffix()
    description = "This is a only_kfserving run."
    pipeline_spec = kfp_server_api.ApiPipelineSpec(pipeline_id=pipeline_id)
    resource_reference_key_id = namespaces[0]
    resource_references=[kfp_server_api.models.ApiResourceReference(
    key=kfp_server_api.models.ApiResourceKey(id=experiment_id, type=kfp_server_api.models.ApiResourceType.EXPERIMENT),
    relationship=kfp_server_api.models.ApiRelationship.OWNER )]
    body = kfp_server_api.ApiRun(name=display_name, description=description, pipeline_spec=pipeline_spec, resource_references=resource_references) # ApiRun | 
    try:
        # Creates a new run.
        run_api_response = run_api_instance.create_run(body)
        run_id = run_api_response.run.id # str | The ID of the run to be retrieved.
    except ApiException as e:
        print("Exception when calling RunServiceApi->create_run: %s\\n" % e)
    
    Completed_flag = False
    polling_interval = 10  # Time in seconds between polls

    while not Completed_flag:
        try:
            time.sleep(1)
            # Finds a specific run by ID.
            api_instance = run_api_instance.get_run(run_id)
            output = api_instance.pipeline_runtime.workflow_manifest
            output = json.loads(output)

            try:
                nodes = output['status']['nodes']
                conditions = output['status']['conditions']  # Confirm completion.

            except KeyError:
                nodes = {}
                conditions = []

            Completed_flag = conditions[1]['status'] if len(conditions) > 1 else False

        except ApiException as e:
            print("Exception when calling RunServiceApi->get_run: %s\\n" % e)
            break

        if not Completed_flag:
            print("Pipeline is still running. Waiting...")
            time.sleep(polling_interval-1)
     
    found_status = False  # Add a variable to track if the status has been found

    def find_status(node):
        global found_status  # Declare the variable as a global variable

        if not found_status:  # If the status has not been found yet
            if isinstance(node, dict):
                if 'status' in node:
                    status = node['status']
                    if 'phase' in status:
                        if status['phase'] == 'Failed':
                            print("status: Failed Deployment!")
                        elif status['phase'] == 'Succeeded':
                            print("status: Successful Deployment!")
                    for key, value in node.items():
                        result = find_status(value)
                        if result:
                            return result
            elif isinstance(node, list):
                for item in node:
                    result = find_status(item)
                    if result:
                        return result

        return None

    # Call the function to find and print the status
    find_status(output)
`



module.exports = {
    IMPORTS,
    DATA_UPLOAD,
    DATASETS,
    DATASETS_CUSTOMIZED,
    DATAPROCESS,
    FILLNANBYMEAN,
    FILLNANBYMEDIAN,
    FILLNANBYVALUE,
    LABELENCODING,
    ONEHOTNCODING,
    STANDARDLIZATION,
    NORMALIZATION,
    DATA_RESHAPE1D,
    DATA_RESHAPE2D,
    QNN,
    UPLOAD_QNN,
    DEPLOY_KSERVE,
    ZZ_MAP,
    PAULI_MAP,
    RA,
    EF,
};