{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Cloud Functions to support event-based triggering of Kubeflow Pipelines\n",
    "\n",
    "This notebook shows how you can run a Kubeflow Pipeline from a [Google Cloud Function](https://cloud.google.com/functions/docs/), thus providing a way for Pipeline runs to be triggered by events (in the interim before this is supported by Pipelines itself).  \n",
    "\n",
    "In this example, the function is triggered by the addition of or update to a file in a [Google Cloud Storage](https://cloud.google.com/storage/) (GCS) bucket, but Cloud Functions can have other triggers too (including [Pub/Sub](https://cloud.google.com/pubsub/docs/)-based triggers).\n",
    "\n",
    "The example is Google Cloud Platform (GCP)-specific, and requires an [IAP](https://cloud.google.com/iap/)-enabled Kubeflow install on GKE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Kubeflow cluster on GKE using IAP\n",
    "\n",
    "Deploy an **IAP-enabled** Kubeflow cluster on GKE.  The [launcher web app](https://deploy.kubeflow.cloud/#/deploy) is recommended. Follow [the instructions](https://www.kubeflow.org/docs/gke/deploy/oauth-setup/) to create an Oauth client ID and secret. \n",
    "\n",
    "Note the client ID, which you'll need later in the notebook to create a Pipelines SDK client. Note the IAP endpoint too, which should be of the form: `https://<deployment-name>.endpoints.<project>.cloud.goog`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple GCF function to test your configuration\n",
    "\n",
    "First we'll generate and deploy a simple GCF function, to test that the basics are properly configured.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a `requirements.txt` file in the `functions` directory, telling GCF that it needs to install the Kubeflow Pipelines SDK as part of the deployment. This will be required for the second function we'll define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions/requirements.txt\n",
    "kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before executing the next cell**, edit it to set the `TRIGGER_BUCKET` environment variable to a Google Cloud Storage bucket ([create a bucket first](https://console.cloud.google.com/storage/browser) if necessary). Do *not* include the `gs://` prefix in the bucket name.\n",
    "\n",
    "We'll deploy the GCF function so that it will trigger on new and updated files (blobs) in this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TRIGGER_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a simple GCF function in the `functions/main.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions/main.py\n",
    "import logging\n",
    "\n",
    "def gcs_test(data, context):\n",
    "  \"\"\"Background Cloud Function to be triggered by Cloud Storage.\n",
    "     This generic function logs relevant data when a file is changed.\n",
    "\n",
    "  Args:\n",
    "      data (dict): The Cloud Functions event payload.\n",
    "      context (google.cloud.functions.Context): Metadata of triggering event.\n",
    "  Returns:\n",
    "      None; the output is written to Stackdriver Logging\n",
    "  \"\"\"\n",
    "\n",
    "  logging.info('Event ID: {}'.format(context.event_id))\n",
    "  logging.info('Event type: {}'.format(context.event_type))\n",
    "  logging.info('Data: {}'.format(data))\n",
    "  logging.info('Bucket: {}'.format(data['bucket']))\n",
    "  logging.info('File: {}'.format(data['name']))\n",
    "  file_uri = 'gs://%s/%s' % (data['bucket'], data['name'])\n",
    "  logging.info('Using file uri: %s', file_uri)\n",
    "\n",
    "  logging.info('Metageneration: {}'.format(data['metageneration']))\n",
    "  logging.info('Created: {}'.format(data['timeCreated']))\n",
    "  logging.info('Updated: {}'.format(data['updated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the GCF function as follows. (You'll need to wait a moment or two for output of the deployment to display in the notebook).  You can also run this command from a notebook terminal window in the `functions` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd functions\n",
    "gcloud functions deploy gcs_test --runtime python37 --trigger-resource ${TRIGGER_BUCKET} --trigger-event google.storage.object.finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've deployed, test your deployment by adding a file to the specified `TRIGGER_BUCKET`.  Then check in the logs viewer panel (https://console.cloud.google.com/logs/viewer) to confirm that the GCF function was triggered and ran correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Pipeline from a GCF function\n",
    "\n",
    "Next, we'll create a GCF function that deploys a Kubeflow Pipeline when triggered.  For this to work, we need to first do two additional things: \n",
    "\n",
    "- give the 'appspot' member account additional permissions,\n",
    "- then add that account as an `IAP-secured Web App User`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the `<your-project>@appspot.gserviceaccount.com` account permissions\n",
    "\n",
    "When you deployed the GCF function above, a member account was automatically created (if it didn't already exist) of the form:\n",
    "`<your-project>@appspot.gserviceaccount.com`.  Find this service account in the\n",
    "[IAM panel of the cloud console](https://console.cloud.google.com/iam-admin/iam). \n",
    "    \n",
    "Then, give the service account 'Service Account Token Creator' permissions.\n",
    "\n",
    "![give the service account service account token creator permissions](https://storage.googleapis.com/amy-jo/images/kfp-deploy/Screenshot_2019-03-10_12_32_54.png)\n",
    "\n",
    "![give the service account service account token creator permissions](https://storage.googleapis.com/amy-jo/images/kfp-deploy/Screenshot_2019-03-10_12_32_32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the new service account as an IAP-secured web app user\n",
    "\n",
    "Next, add the new service account as an IAP-secured Web App User.  Visit [https://console.cloud.google.com/security/iap](https://console.cloud.google.com/security/iap). \n",
    "Click on `istio-system/istio-ingressgateway` (which was set up as part of your Kubeflow installation), then click on **ADD MEMBER**\n",
    "\n",
    "![](https://storage.googleapis.com/amy-jo/images/kf-pls/IAP_webappuser_setup1.png)\n",
    "\n",
    "Add the new service account as an IAP-secured Web App User.\n",
    "\n",
    "![](https://storage.googleapis.com/amy-jo/images/kfp-deploy/Screenshot_2019-03-09_11_30_24x-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create a version of the GCF function that deploys a pipeline.  First, preserve your existing `main.py` in a backup file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd functions\n",
    "mv main.py main.py.bak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, **before executing the next cell**, edit the `HOST` and `CLIENT_ID` variables in the code below. The client ID is the same Oauth client ID you generated for the IAP-based deployment of your Kubeflow cluster. The `HOST` is your IAP endpoint with `/pipeline` appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions/main.py\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# gcloud functions deploy gcs_update_count --runtime python37 \\\n",
    "#--trigger-resource <your_trigger_bucket> \\\n",
    "#--trigger-event google.storage.object.finalize\n",
    "\n",
    "EXPERIMENT_NAME = 'e1'\n",
    "\n",
    "# EDIT THE NEXT TWO LINES for your installation\n",
    "HOST = 'https://<deployment-name>.endpoints.<project>.cloud.goog/pipeline'\n",
    "CLIENT_ID = '<YOUR_IAP_CLIENT_ID>'\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='Sequential',\n",
    "  description='A pipeline with two sequential steps.'\n",
    ")\n",
    "def sequential_pipeline(filename='gs://ml-pipeline-playground/shakespeare1.txt'):\n",
    "  \"\"\"A pipeline with two sequential steps.\"\"\"\n",
    "\n",
    "  op1 = dsl.ContainerOp(\n",
    "     name='filechange',\n",
    "     image='library/bash:4.4.23',\n",
    "     command=['sh', '-c'],\n",
    "     arguments=['echo \"%s\" > /tmp/results.txt' % filename],\n",
    "     file_outputs={'newfile': '/tmp/results.txt'})\n",
    "  op2 = dsl.ContainerOp(\n",
    "     name='echo',\n",
    "     image='library/bash:4.4.23',\n",
    "     command=['sh', '-c'],\n",
    "     arguments=['echo \"%s\"' % op1.outputs['newfile']]\n",
    "     )\n",
    "\n",
    "\n",
    "def gcs_deploy_pipeline(data, context):\n",
    "  \"\"\"Background Cloud Function to be triggered by Cloud Storage.\n",
    "     This generic function logs relevant data when a file is changed.\n",
    "\n",
    "  Args:\n",
    "      data (dict): The Cloud Functions event payload.\n",
    "      context (google.cloud.functions.Context): Metadata of triggering event.\n",
    "  Returns:\n",
    "      None; the output is written to Stackdriver Logging\n",
    "  \"\"\"\n",
    "\n",
    "  logging.info('Event ID: {}'.format(context.event_id))\n",
    "  logging.info('Event type: {}'.format(context.event_type))\n",
    "  logging.info('Data: {}'.format(data))\n",
    "  logging.info('Bucket: {}'.format(data['bucket']))\n",
    "  logging.info('File: {}'.format(data['name']))\n",
    "  file_uri = 'gs://%s/%s' % (data['bucket'], data['name'])\n",
    "  logging.info('Using file uri: %s', file_uri)\n",
    "\n",
    "  logging.info('Metageneration: {}'.format(data['metageneration']))\n",
    "  logging.info('Created: {}'.format(data['timeCreated']))\n",
    "  logging.info('Updated: {}'.format(data['updated']))\n",
    "\n",
    "\n",
    "  logging.info('attempting to launch pipeline run.')\n",
    "  ts = int(datetime.datetime.utcnow().timestamp() * 100000)\n",
    "  client = kfp.Client(\n",
    "      host=HOST, client_id=CLIENT_ID)\n",
    "  compiler.Compiler().compile(sequential_pipeline, '/tmp/sequential.tar.gz')\n",
    "  exp = client.create_experiment(name=EXPERIMENT_NAME)  # this is a 'get or create' op\n",
    "  res = client.run_pipeline(exp.id, 'sequential_' + str(ts), '/tmp/sequential.tar.gz',\n",
    "                              params={'filename': file_uri})\n",
    "  # alternately, if you want to launch a run using an already-existing pipeline, you can pass\n",
    "  # the pipeline ID as follows, replacing 'your-pipeline-id' with the actual ID.\n",
    "  # res = client.run_pipeline(exp.id, 'your-pipeline-name',\n",
    "  #                           params={...}, pipeline_id='your-pipeline-id')\n",
    "  logging.info(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we're passing the name of the added or updated GCS file (the file that triggered the GCF function) to our (very simple) example pipeline as an input parameter.\n",
    "\n",
    "Next, deploy the GCF function that you just created. As before, it will take a moment or two for the results of the deployment to display in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd functions\n",
    "gcloud functions deploy gcs_deploy_pipeline --runtime python37 --trigger-resource ${TRIGGER_BUCKET} --trigger-event google.storage.object.finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another file to your `TRIGGER_BUCKET`. This time you should see both GCF functions triggered. The `gcs_deploy_pipeline` function will deploy the pipeline. You'll be able to see it running at your Kubeflow cluster's IAP endpoint, `https://<deployment-name>.endpoints.<project>.cloud.goog/pipeline`, under the given Pipelines Experiment (`e1` as default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding and using the ID of an existing pipeline to launch a run\n",
    "\n",
    "In the scenario above, we ran a pipeline by uploading its compiled archive.\n",
    "Sometimes, you might want to trigger a run of an existing pipeline, that has already been uploaded previously (e.g., via the `upload_pipeline` method, or via the web UI). \n",
    "\n",
    "For this, you need the ID of the pipeline.  You can get it via the `list_pipelines` method, filtering on the name of the pipeline that you want.\n",
    "\n",
    "See one of the other notebooks in this directory, e.g. [kfp_remote_deploy-IAP.ipynb](kfp_remote_deploy-IAP.ipynb), for an example of defining and using a utility function that uses `list_pipelines` to find a pipeline's ID given its name. You would add that code to your `functions/main.py` file, then re-deploy it to GCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communicating pipeline parameters via a GCF trigger file\n",
    "\n",
    "In the simple example above, we showed how to pass the name of the added/updated GCS file — obtained from the GCF function arguments— as a pipeline parameter.\n",
    "More realistically, you may often want to communicate information via the GCS file *contents*, which can then be extracted and passed as pipeline parameter(s).\n",
    "\n",
    "For example, suppose you are [exporting data labeled by the AI Platform Data Labeling Service](https://cloud.google.com/data-labeling/docs/export#datalabel-example-python).  As part of the function you write to **export the labeled data to GCS**, you could include a call to the `write_export_info` utility function below (or something similar), where you write the location of the exported data to the `$TRIGGER_BUCKET` you set up for your GCS function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_export_info(bucket_name, file_name, export_info_string):\n",
    "  \"\"\"Utility function to write a string to a GCS file.\"\"\"\n",
    "  from google.cloud import storage\n",
    "\n",
    "  storage_client = storage.Client()\n",
    "  bucket = storage_client.get_bucket(bucket_name)  # e.g., a GCF 'trigger' bucket. Don't include the 'gs://'-- just the name\n",
    "  blob = bucket.blob(file_name)  # don't include a leading /\n",
    "  blob.upload_from_string(export_info_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in your GCF function, you can add code to read the contents of the GCS trigger file. For example, you could add a function similar to this to your GCF `main.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_export_info(bucket_name, file_name):\n",
    "  storage_client = storage.Client()\n",
    "  bucket = storage_client.get_bucket(bucket_name)\n",
    "  blob = bucket.blob(file_name)\n",
    "  return blob.download_as_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, do any necessary parsing of that string to extract parameter information, and make a call to `run_pipeline` using those extracted parameters.  That might look something like this snippet, if you were editing the `main.py` we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcs_deploy_pipeline(data, context):\n",
    "  ...\n",
    "  export_info = read_export_info(data['bucket'], data['name'])\n",
    "  ...\n",
    "  client.run_pipeline(exp.id, 'sequential_' + str(ts), '/tmp/sequential.tar.gz',\n",
    "                            params={'filename': str(export_info)})\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "Copyright 2019, Google, LLC.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
