{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Cloud Functions to support event-based triggering of Kubeflow Pipelines\n",
    "\n",
    "This notebook shows how you can run a Kubeflow Pipeline from a [Google Cloud Function](https://cloud.google.com/functions/docs/), thus providing a way for Pipeline runs to be triggered by events (in the interim before this is supported by Pipelines itself).  \n",
    "\n",
    "In this example, the function is triggered by the addition of or update to a file in a [Google Cloud Storage](https://cloud.google.com/storage/) (GCS) bucket, but Cloud Functions can have other triggers too (including [Pub/Sub](https://cloud.google.com/pubsub/docs/)-based triggers).\n",
    "\n",
    "The example is Google Cloud Platform (GCP)-specific, and requires an [IAP](https://cloud.google.com/iap/)-enabled Kubeflow install on GKE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Kubeflow cluster on GKE using IAP\n",
    "\n",
    "Deploy an **IAP-enabled** Kubeflow cluster on GKE.  The [launcher web app](https://deploy.kubeflow.cloud/#/deploy) is recommended. Follow [the instructions](https://www.kubeflow.org/docs/started/getting-started-gke/#create-oauth-client-credentials) to create an Oauth client ID and secret. \n",
    "\n",
    "Note the client ID, which you'll need later in the notebook to create a Pipelines SDK client. Note the IAP endpoint too, which should be of the form: `https://<deployment-name>.endpoints.<project>.cloud.goog`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple GCF function to test your configuration\n",
    "\n",
    "First we'll generate and deploy a simple GCF function, to test that the basics are properly configured.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions/requirements.txt\n",
    "https://storage.googleapis.com/ml-pipeline/release/0.1.12/kfp.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before executing the next cell**, edit it to set the `TRIGGER_BUCKET` environment variable to a Google Cloud Storage bucket ([create a bucket first](https://console.cloud.google.com/storage/browser) if necessary). Do *not* include the `gs://` prefix in the bucket name.\n",
    "\n",
    "We'll deploy the GCF function so that it will trigger on new and updated files (blobs) in this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TRIGGER_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a simple GCF function in the `functions/main.py` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions/main.py\n",
    "import logging\n",
    "\n",
    "def gcs_test(data, context):\n",
    "  \"\"\"Background Cloud Function to be triggered by Cloud Storage.\n",
    "     This generic function logs relevant data when a file is changed.\n",
    "\n",
    "  Args:\n",
    "      data (dict): The Cloud Functions event payload.\n",
    "      context (google.cloud.functions.Context): Metadata of triggering event.\n",
    "  Returns:\n",
    "      None; the output is written to Stackdriver Logging\n",
    "  \"\"\"\n",
    "\n",
    "  logging.info('Event ID: {}'.format(context.event_id))\n",
    "  logging.info('Event type: {}'.format(context.event_type))\n",
    "  logging.info('Data: {}'.format(data))\n",
    "  logging.info('Bucket: {}'.format(data['bucket']))\n",
    "  logging.info('File: {}'.format(data['name']))\n",
    "  file_uri = 'gs://%s/%s' % (data['bucket'], data['name'])\n",
    "  logging.info('Using file uri: %s', file_uri)\n",
    "\n",
    "  logging.info('Metageneration: {}'.format(data['metageneration']))\n",
    "  logging.info('Created: {}'.format(data['timeCreated']))\n",
    "  logging.info('Updated: {}'.format(data['updated']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the GCF function as follows. (You'll need to wait a moment or two for output of the deployment to display in the notebook).  You can also run this command from a notebook terminal window in the `functions` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd functions\n",
    "gcloud functions deploy gcs_test --runtime python37 --trigger-resource ${TRIGGER_BUCKET} --trigger-event google.storage.object.finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've deployed, test your deployment by adding a file to the specified `TRIGGER_BUCKET`.  Then check in the logs viewer panel (https://console.cloud.google.com/logs/viewer) to confirm that the GCF function was triggered and ran correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a Pipeline from a GCF function\n",
    "\n",
    "Next, we'll create a GCF function that deploys a Kubeflow Pipeline when triggered.  For this to work, we need to first do two additional things: \n",
    "\n",
    "- give the 'appspot' member account additional permissions,\n",
    "- then add that account as an `IAP-secured Web App User`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the `<your-project>@appspot.gserviceaccount.com` account permissions\n",
    "\n",
    "When you deployed the GCF function above, a member account was automatically created (if it didn't already exist) of the form:\n",
    "`<your-project>@appspot.gserviceaccount.com`.  Find this service account in the\n",
    "[IAM panel of the cloud console](https://console.cloud.google.com/iam-admin/iam). \n",
    "    \n",
    "Then, give the service account 'Service Account Token Creator' permissions.\n",
    "\n",
    "![give the service account service account token creator permissions](https://storage.googleapis.com/amy-jo/images/kfp-deploy/Screenshot_2019-03-10_12_32_54.png)\n",
    "\n",
    "![give the service account service account token creator permissions](https://storage.googleapis.com/amy-jo/images/kfp-deploy/Screenshot_2019-03-10_12_32_32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the new service account as an IAP-secured web app user\n",
    "\n",
    "Next, add the new service account as an IAP-secured Web App User.  Visit [https://console.cloud.google.com/security/iap](https://console.cloud.google.com/security/iap). \n",
    "Click on 'kubeflow/envoy', then click on **ADD MEMBER**\n",
    "\n",
    "![](https://storage.googleapis.com/amy-jo/images/kfp-deploy/Screenshot_2019-03-09_11_29_14.png)\n",
    "\n",
    "Add the new service account as an IAP-secured Web App User.\n",
    "\n",
    "![](https://storage.googleapis.com/amy-jo/images/kfp-deploy/Screenshot_2019-03-09_11_30_24x-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create a version of the GCF function that deploys a pipeline.  First, preserve your existing `main.py` in a backup file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd functions\n",
    "mv main.py main.py.bak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, **before executing the next cell**, edit the `HOST` and `CLIENT_ID` variables in the code below. The client ID is the same Oauth client ID you generated for the IAP-based deployment of your Kubeflow cluster. The `HOST` is your IAP endpoint with `/pipeline` appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile functions/main.py\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# gcloud functions deploy gcs_update_count --runtime python37 \\\n",
    "#--trigger-resource <your_trigger_bucket> \\\n",
    "#--trigger-event google.storage.object.finalize\n",
    "\n",
    "EXPERIMENT_NAME = 'e1'\n",
    "\n",
    "# EDIT THE NEXT TWO LINES for your installation\n",
    "HOST = 'https://<deployment-name>.endpoints.<project>.cloud.goog/pipeline'\n",
    "CLIENT_ID = '<YOUR_IAP_CLIENT_ID>'\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='Sequential',\n",
    "  description='A pipeline with two sequential steps.'\n",
    ")\n",
    "def sequential_pipeline(filename='gs://ml-pipeline-playground/shakespeare1.txt'):\n",
    "  \"\"\"A pipeline with two sequential steps.\"\"\"\n",
    "\n",
    "  op1 = dsl.ContainerOp(\n",
    "     name='filechange',\n",
    "     image='library/bash:4.4.23',\n",
    "     command=['sh', '-c'],\n",
    "     arguments=['echo \"%s\" > /tmp/results.txt' % filename],\n",
    "     file_outputs={'newfile': '/tmp/results.txt'})\n",
    "  op2 = dsl.ContainerOp(\n",
    "     name='echo',\n",
    "     image='library/bash:4.4.23',\n",
    "     command=['sh', '-c'],\n",
    "     arguments=['echo \"%s\"' % op1.outputs['newfile']]\n",
    "     )\n",
    "\n",
    "\n",
    "def gcs_update_count(data, context):\n",
    "  \"\"\"Background Cloud Function to be triggered by Cloud Storage.\n",
    "     This generic function logs relevant data when a file is changed.\n",
    "\n",
    "  Args:\n",
    "      data (dict): The Cloud Functions event payload.\n",
    "      context (google.cloud.functions.Context): Metadata of triggering event.\n",
    "  Returns:\n",
    "      None; the output is written to Stackdriver Logging\n",
    "  \"\"\"\n",
    "\n",
    "  logging.info('Event ID: {}'.format(context.event_id))\n",
    "  logging.info('Event type: {}'.format(context.event_type))\n",
    "  logging.info('Data: {}'.format(data))\n",
    "  logging.info('Bucket: {}'.format(data['bucket']))\n",
    "  logging.info('File: {}'.format(data['name']))\n",
    "  file_uri = 'gs://%s/%s' % (data['bucket'], data['name'])\n",
    "  logging.info('Using file uri: %s', file_uri)\n",
    "\n",
    "  logging.info('Metageneration: {}'.format(data['metageneration']))\n",
    "  logging.info('Created: {}'.format(data['timeCreated']))\n",
    "  logging.info('Updated: {}'.format(data['updated']))\n",
    "\n",
    "\n",
    "  logging.info('attempting to launch pipeline run.')\n",
    "  ts = int(datetime.datetime.utcnow().timestamp() * 100000)\n",
    "  client = kfp.Client(\n",
    "      host=HOST, client_id=CLIENT_ID)\n",
    "  compiler.Compiler().compile(sequential_pipeline, '/tmp/sequential.tar.gz')\n",
    "  exp = client.create_experiment(name=EXPERIMENT_NAME)  # this is a 'get or create' op\n",
    "  res = client.run_pipeline(exp.id, 'sequential_' + str(ts), '/tmp/sequential.tar.gz',\n",
    "                              params={'filename': file_uri})\n",
    "  logging.info(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, deploy the new GCF function. As before, it will take a moment or two for the results of the deployment to display in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd functions\n",
    "gcloud functions deploy gcs_update_count --runtime python37 --trigger-resource ${TRIGGER_BUCKET} --trigger-event google.storage.object.finalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another file to your `TRIGGER_BUCKET`. This time you should see both GCF functions triggered. The `gcs_update_count` function will deploy the pipeline. You'll be able to see it running at your Kubeflow cluster's IAP endpoint, `https://<deployment-name>.endpoints.<project>.cloud.goog/pipeline`, under the given Pipelines Experiment (`e1` as default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------\n",
    "Copyright 2019, Google, LLC.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
