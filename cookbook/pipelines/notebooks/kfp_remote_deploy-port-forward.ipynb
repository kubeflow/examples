{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy and run a Kubeflow Pipeline from outside the Kubeflow cluster: non-IAP version\n",
    "\n",
    "This notebook shows how to deploy and run a [Kubeflow](https://kubeflow.org) Pipeline from outside the Kubeflow cluster.  \n",
    "\n",
    "In contrast to the [`kfp_remote_deploy-IAP.ipynb`](kfp_remote_deploy-IAP.ipynb) notebook, this example does not require that the cluster has been set up to use Google Cloud Platform's [Identity-Aware Proxy (IAP)](https://cloud.google.com/iap/). \n",
    "\n",
    "However, it does assume that you're able to set up a port-forward to the Pipelines UI. For Kubeflow installations that include Pipelines (including the ['lightweight' Kubeflow Pipelines installation](https://github.com/kubeflow/pipelines/tree/master/manifests/kustomize)), you should be able to connect directly to the Pipelines UI as follows, replacing `8080` with the local port you'd like to use.\n",
    "\n",
    "```sh\n",
    "kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n",
    "```\n",
    "For generality of example, we'll use this port-forwarding command for this notebook.\n",
    "\n",
    "> **Important Note**: the command above will not give you access to the rest of the Kubeflow Dashboard. But, it is a straightforward way to connect in order to deploy pipelines and make other Pipelines API calls.\n",
    "\n",
    "Alternately, if your Kubeflow cluster has been set up to support port-forwarding, as with the generic [installation instructions here](https://www.kubeflow.org/docs/started/k8s/kfctl-k8s-istio/), you may be able to port-forward via istio to the Kubeflow Dashboard as follows: `kubectl -n istio-system port-forward svc/istio-ingressgateway 8080:80`.\n",
    "\n",
    "For simplicity of example we do assume a cluster running on [Kubernetes Engine](https://cloud.google.com/kubernetes-engine/) (GKE). However, this basic approach should work for Kubeflow installed on non-GKE  Kubernetes-conformant clusters as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and configuration\n",
    "\n",
    "\n",
    "### Deploy a Kubeflow cluster on GKE\n",
    "\n",
    "Deploy a [Kubeflow](https://kubeflow.org) cluster on GKE.  The [launcher web app](https://deploy.kubeflow.cloud/#/deploy) is recommended for ease of use. \n",
    "You can also deploy from the command line using the `kfctl` utility, as [described here](https://www.kubeflow.org/docs/gke/deploy/deploy-cli/).\n",
    "\n",
    "Once the GKE cluster is up and running, visit https://console.cloud.google.com/kubernetes/list and click on the **Connect** button to the right of your Kubeflow cluster. Copy the given command-line access snippet, which should look like:\n",
    "\n",
    "```\n",
    "gcloud container clusters get-credentials <your-cluster-name> --zone <your-zone> --project <your-project>\n",
    "```\n",
    "You'll need this for the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the notebook example\n",
    "\n",
    "### Run the example in an AI Platform Notebook\n",
    "\n",
    "\n",
    "Visit [https://console.cloud.google.com/mlengine/notebooks/instances](https://console.cloud.google.com/mlengine/notebooks/instances) and create a **NEW INSTANCE** (or you can use an existing instance if you prefer).\n",
    "\n",
    "Once the instance is up and running, click on **OPEN JUPYTERLAB**, and upload this notebook.\n",
    "\n",
    "Under **File** > **New**, start up a new **Terminal** tab.  In the terminal window, run the `gcloud container clusters get-credentials ...` command described above.\n",
    "This will auth `kubectl` to connect to your Kubeflow cluster. \n",
    "\n",
    "Then, port-forward to the Kubeflow dashboard by running the following command in the terminal window. \n",
    "\n",
    "```\n",
    "kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8089:80\n",
    "```\n",
    "\n",
    "(Often the convention is to use port 8080, but in the AI Platform notebook environment that port is already taken, so we're using another, `8089`.)\n",
    "\n",
    "### Run the example in a local Jupyter installation\n",
    "\n",
    "To run this notebook locally, you'll need Jupyter and Python 3 installed.  \n",
    "\n",
    "Then, in your local environment, run the `gcloud container clusters get-credentials ...` command described above.\n",
    "This will auth `kubectl` to connect to your Kubeflow cluster. \n",
    "\n",
    "Then, port-forward to the Kubeflow dashboard by running the `kubectl port-forward ...` command of the previous section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code\n",
    "\n",
    "Now we're ready to run the example code.\n",
    "\n",
    "First, install the Kubeflow Pipelines SDK. If you get import errors, **you may need to restart your notebook kernel after you do the installation**.    \n",
    "Make sure you're using Python 3. \n",
    "If you're running this notebook locally within a Conda environment, you may need to change `pip3` to `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kfp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a (very) simple example pipeline to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Sequential',\n",
    "  description='A pipeline with two sequential steps.'\n",
    ")\n",
    "def sequential_pipeline(filename='gs://ml-pipeline-playground/shakespeare1.txt'):\n",
    "  \"\"\"A pipeline with two sequential steps.\"\"\"\n",
    "\n",
    "  op1 = dsl.ContainerOp(\n",
    "     name='getfilename',\n",
    "     image='library/bash:4.4.23',\n",
    "     command=['sh', '-c'],\n",
    "     arguments=['echo \"%s\" > /tmp/results.txt' % filename],\n",
    "     file_outputs={'newfile': '/tmp/results.txt'})\n",
    "  op2 = dsl.ContainerOp(\n",
    "     name='echo',\n",
    "     image='library/bash:4.4.23',\n",
    "     command=['sh', '-c'],\n",
    "     arguments=['echo \"%s\"' % op1.outputs['newfile']]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create an instance of the Kubeflow Pipelines client. It will connect via the port-forward connection you set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = int(datetime.datetime.utcnow().timestamp() * 100000)\n",
    "client = kfp.Client('127.0.0.1:8089/pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the pipeline, and create a Pipelines `Experiment`. (If you're running in an AI Platform notebook, the generated 'experiment' link will not work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(sequential_pipeline, '/tmp/sequential.tar.gz')\n",
    "exp = client.create_experiment(name='sequential')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the pipeline.\n",
    "If you're running this example in an AI Platform notebook, the generated 'run' link will not work. Instead, to view the Kubeflow and Kubeflow Pipelines dashboards, you can set up an additional port-forward from your local machine (a bit more info is [here](https://www.kubeflow.org/docs/other-guides/accessing-uis/)), or from the GCP [cloud shell](https://cloud.google.com/shell/docs/using-web-preview) using its 'Web Preview' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =  client.run_pipeline(exp.id, 'sequential_' + str(ts), \n",
    "                           '/tmp/sequential.tar.gz',\n",
    "                          )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding and using the ID of an existing pipeline to launch a run\n",
    "\n",
    "In the scenario above, we ran a pipeline by uploading its compiled archive.\n",
    "Sometimes, you might want to trigger a run of an existing pipeline, that has already been uploaded previously (e.g., via the `upload_pipeline` method, or via the web UI). \n",
    "\n",
    "For this, you need the ID of the pipeline.  You can get it via the `list_pipelines` method, filtering on the name of the pipeline that you want.\n",
    "\n",
    "Here is an example wrapper method that takes as args the pipeline name and the client object, and returns the ID of that pipeline if it's in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_id(name, client):\n",
    "  pl_id = None\n",
    "  page_size = 100\n",
    "  page_token = ''\n",
    "  while True:\n",
    "    res = client.list_pipelines(page_size=page_size, page_token=page_token)\n",
    "    pl_list = res.pipelines\n",
    "    for pl in pl_list:\n",
    "      if pl.name == name:\n",
    "        pl_id = pl.id\n",
    "        return pl_id\n",
    "    page_token = res.next_page_token\n",
    "    if not page_token:\n",
    "      break\n",
    "  return pl_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this helper method to trigger a pipeline run based on its ID. The pipeline with name \"[Sample] Basic - Parallel execution\" should exist as a sample in your Kubeflow installation, so let's use that as our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = '[Sample] Basic - Parallel execution'\n",
    "pipeline_id = get_pipeline_id(name, client)\n",
    "if pipeline_id:\n",
    "  print(\"using pipeline ID: %s\" pipeline_id)\n",
    "  res = client.run_pipeline(exp.id, 'parallelexec' + str(ts),\n",
    "                            # params={...},\n",
    "                            pipeline_id=pipeline_id)\n",
    "  else:\n",
    "    print(\"Could not find Pipeline ID from name %s\" % name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019, Google, LLC.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
