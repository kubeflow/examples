# Launch a distributed object detection training job
## Requirements

 - Docker
 - Docker Registry
 - Object Detection Training Docker Image

Build the TensorFlow object detection training image, or use the pre-built image `lcastell/pets_object_detection` in Docker hub.

## To build the image:
First copy the Dockerfile.training tfile from `./docker` directory into your $HOME path
```
# from your $HOME directory
docker build --pull -t $USER/pets_object_detection -f ./Dockerfile.training .
```

### Push the image to your docker registry
```
# from your $HOME directory
docker tag  $USER/pets_object_detection  <your_server:your_port>/pets_object_detection
docker push <your_server:your_port>/pets_object_detection
```

## Create  training TF-Job deployment and launching it

```
# from the ks-app directory

PIPELINE_CONFIG_PATH="${MOUNT_PATH}/faster_rcnn_resnet101_pets.config"
TRAINING_DIR="${MOUNT_PATH}/train"

<<<<<<< HEAD
ks param set tf-training-job image ${OBJ_DETECTION_IMAGE}
ks param set tf-training-job mountPath ${MOUNT_PATH}
ks param set tf-training-job pvc ${PVC}
ks param set tf-training-job numPs 1
ks param set tf-training-job numWorkers 1
ks param set tf-training-job pipelineConfigPath ${PIPELINE_CONFIG_PATH}
ks param set tf-training-job trainDir ${TRAINING_DIR}

ks apply ${ENV} -c tf-training-job
=======
Generate the ksonnet component using the tf-job prototype
```
# from the my-kubeflow directory
ks generate tf-job pets-training --name=pets-traning \
--namespace=kubeflow \
--image=<your_server:your_port>/pets_object_detection \
--num_masters=1 \
--num_workers= 1 \
--num_ps= 1
```
Dump the generated component into a K8s deployment manifest file.
```
ks show nocloud -c pets-training > pets-training.yaml
```
Add the volume mounts information at the end manifest file. We will be mounting `/pets_data` path to all the containers so they can pull the data for the training job
```
vim pets-training.yaml
```
Add the following to the template.spec:
```
volumes:
  - name: pets-data
    persistentVolumeClaim:
      claimName: pets-data-claim
```
Add the following to the container properties:
```
volumeMounts:
- mountPath: "/pets_data"
  name: pets-data
>>>>>>> adding batch-predict on GPU example
```

For GPU support set the `numGpu` param like:
```
# from the ks-app directory
ks param set tf-training-job numGpu 1
```

Here is a quick description for the `tf-training-job` component parameters:

- `image` string, docker image to use
- `mountPath` string, Volume mount path
- `numGpu` number, optional param, default to 0
- `numPs` number, Number of Parameter servers to use
- `numWorkers` number, Number of workers to use
- `pipelineConfigPath` string, the path to the pipeline config file in the volume mount
- `pvc` string, Persistent Volume Claim name to use
- `trainDir` string, Directory where the training outputs will be saved

To see the default values for the `tf-training-job` component params, please take a look at the [params.libsonnet](./ks-app/params.libsonnet) file.

## Next
[Monitor your job](monitor_job.md)
