{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kaggle Featured Prediction Competition: H&M Personalized Fashion Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this [competition](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations), product recommendations have to be done based on previous purchases. There's a whole range of data available including customer meta data, product meta data, and meta data that spans from simple data, such as garment type and customer age, to text data from product descriptions, to image data from garment images.\n",
    "\n",
    "In this notebook we will be working with implicit's ALS library for our recommender systems. Please do check out the [docs](https://benfred.github.io/implicit/index.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install necessary packages\n",
    "\n",
    "We can install the necessary package by either running `pip install --user <package_name>` or include everything in a `requirements.txt` file and run `pip install --user -r requirements.txt`. We have put the dependencies in a `requirements.txt` file so we will use the former method.\n",
    "\n",
    "Restart the kernel after installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download Data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Download relevant data from kaggle by running the below code cell. Follow the initial steps information mentioned in Github README.md to get the Kaggle username and key for authentication of Kaggle Public API. There's no need of secret to be created for the following step. The credentials will be present in the kaggle.json file. This cell needs to be run before starting Kale pipeline from  Kale deployment panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading customers.csv.zip to /home/jovyan/examples-1/h-and-m-fash-rec-kaggle-competition/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.9M/97.9M [00:01<00:00, 80.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading transactions_train.csv.zip to /home/jovyan/examples-1/h-and-m-fash-rec-kaggle-competition/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 584M/584M [00:08<00:00, 69.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading articles.csv.zip to /home/jovyan/examples-1/h-and-m-fash-rec-kaggle-competition/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.26M/4.26M [00:00<00:00, 240MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sample_submission.csv.zip to /home/jovyan/examples-1/h-and-m-fash-rec-kaggle-competition/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50.3M/50.3M [00:00<00:00, 81.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/jovyan/examples-1/h-and-m-fash-rec-kaggle-competition/data/customers.csv.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Get the Kaggle Username and password from the kaggle.json file\n",
    "# and paste it in place of KAGGLE_USERNAME AND KAGGLE_KEY\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = \"KAGGLE_USERNAME\"\n",
    "os.environ['KAGGLE_KEY'] = \"KAGGLE_KEY\"\n",
    "\n",
    "path = \"data/\"\n",
    "\n",
    "os.chdir(os.getcwd())\n",
    "os.system(\"mkdir \" + path)\n",
    "os.chdir(path)\n",
    "\n",
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "        \n",
    "# Download the required files individually. You can also choose to download the entire dataset if you want to work with image data as well. The files will be in downloaded   \n",
    "api.competition_download_file('h-and-m-personalized-fashion-recommendations','customers.csv')\n",
    "api.competition_download_file('h-and-m-personalized-fashion-recommendations','transactions_train.csv')\n",
    "api.competition_download_file('h-and-m-personalized-fashion-recommendations','articles.csv')\n",
    "api.competition_download_file('h-and-m-personalized-fashion-recommendations','sample_submission.csv')   \n",
    "\n",
    "# Get the path of the directory where the \n",
    "path_dir = os.getcwd()\n",
    "\n",
    "from zipfile import ZipFile \n",
    "\n",
    "# Extracting all files from individual zip files\n",
    "zipfile1 = ZipFile(path_dir + '/customers.csv.zip', 'r')\n",
    "zipfile1.extract(\"customers.csv\")\n",
    "zipfile1.close()\n",
    "    \n",
    "zipfile2 = ZipFile(path_dir + '/transactions_train.csv.zip', 'r')\n",
    "zipfile2.extract(\"transactions_train.csv\")\n",
    "zipfile2.close()\n",
    "    \n",
    "zipfile3 = ZipFile(path_dir + '/articles.csv.zip', 'r')\n",
    "zipfile3.extract(\"articles.csv\")\n",
    "zipfile3.close()\n",
    "    \n",
    "zipfile4 = ZipFile(path_dir + '/sample_submission.csv.zip', 'r')\n",
    "zipfile4.extract(\"sample_submission.csv\")\n",
    "zipfile4.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import implicit \n",
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:load_and_preprocess_data"
    ]
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "train_data_filepath = path + \"transactions_train.csv\"\n",
    "article_metadata_filepath = path + \"articles.csv\"\n",
    "customer_metadata_filepath = path + \"customers.csv\"\n",
    "test_data_filepath = path + \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_filepath)\n",
    "test_data = pd.read_csv(test_data_filepath)\n",
    "customer_data = pd.read_csv(customer_metadata_filepath)\n",
    "article_data = pd.read_csv(article_metadata_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "customer_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "article_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will be dropping t_dat, sales_channel and price as this won't be part of the recommendation system we will be building \n",
    "train_data.drop(['t_dat','sales_channel_id','price'], axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:"
    ]
   },
   "outputs": [],
   "source": [
    "# create a new purchase count column that would gives us count of every article bought by the customers\n",
    "X = train_data.groupby(['customer_id', 'article_id'])['article_id'].count().reset_index(name = \"purchase_count\") \n",
    "\n",
    "# Getting unique number of customers and articles using the customer and article metadata data files\n",
    "unique_customers = customer_data['customer_id'].unique()\n",
    "unique_articles = article_data['article_id'].unique()\n",
    "\n",
    "# length of the customers and articles\n",
    "n_customers = len(unique_customers)\n",
    "n_articles = len(unique_articles)\n",
    "\n",
    "# Create a mapping for customer_id to convert it from an object column to an int column for the sparse matrix creation\n",
    "customer_id_dict = {unique_customers[i]:i  for i in range(len(unique_customers))}\n",
    "reverse_customer_id_dict = {i:unique_customers[i] for i in range(len(unique_customers))} \n",
    "numeric_cus_id = []\n",
    "for i in range(len(X['customer_id'])):\n",
    "    numeric_cus_id.append(customer_id_dict.get(X['customer_id'][i]))\n",
    "X['customer_id'] = numeric_cus_id\n",
    "\n",
    "# Create a mapping for article_id so that the sparse matrix creation doesn't get large enough due to long int values of article_ids\n",
    "article_id_dict = {unique_articles[i]:i  for i in range(len(unique_articles))}\n",
    "reverse_article_id_dict = {i:unique_articles[i] for i in range(len(unique_articles))}\n",
    "numeric_art_id = []\n",
    "for i in range(len(X['article_id'])):\n",
    "    numeric_art_id.append(article_id_dict.get(X['article_id'][i]))\n",
    "X['article_id'] = numeric_art_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sparse Matrix Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:sparse_matrix_creation",
     "prev:load_and_preprocess_data"
    ]
   },
   "outputs": [],
   "source": [
    "# Constructing sparse matrices for alternating least squares algorithm    \n",
    "sparse_user_item_coo = sparse.coo_matrix((X.purchase_count, (X.customer_id, X.article_id)), shape = (n_customers, n_articles))\n",
    "sparse_user_item_csr = sparse.csr_matrix((X['purchase_count'], (X['customer_id'], X['article_id'])), shape = (n_customers, n_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "sparse_user_item_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:train_model",
     "prev:sparse_matrix_creation"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters for the model\n",
    "als_params = dict(\n",
    "    factors = 200,         # number of latent factors - try between 50 to 1000\n",
    "    regularization = 0.01, # regularization factor - try between 0.001 to 0.2\n",
    "    iterations = 5,        # iterations            - try between 2 to 100\n",
    ")\n",
    "\n",
    "# initialize a model\n",
    "model = implicit.als.AlternatingLeastSquares(**als_params)\n",
    "\n",
    "# train the model on a sparse matrix of user/item/confidence weights    \n",
    "model.fit(sparse_user_item_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:predictions",
     "prev:sparse_matrix_creation",
     "prev:train_model",
     "prev:load_and_preprocess_data"
    ]
   },
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "count = 0\n",
    "for cust_id in test_data.customer_id:\n",
    "    cust_id = customer_id_dict.get(cust_id)\n",
    "#     if(cust_id!=None):    \n",
    "    recommendations = model.recommend(cust_id, sparse_user_item_csr[cust_id],10)\n",
    "    result=[]\n",
    "    for i in range(len(recommendations[0])):\n",
    "        val = reverse_article_id_dict.get(recommendations[0][i])\n",
    "        result.append(val)  \n",
    "    predictions.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data['prediction'] = predictions\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "test_data.to_csv('data/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "gcr.io/arrikto/jupyter-kale-py36@sha256:dd3f92ca66b46d247e4b9b6a9d84ffbb368646263c2e3909473c3b851f3fe198",
   "experiment": {
    "id": "new",
    "name": "hm-fash-recomm"
   },
   "experiment_name": "hm-fash-recomm",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "",
   "pipeline_name": "predict-hm-purchases-kale-1",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "hm-test-workspace-qfnn5",
     "size": 50,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
