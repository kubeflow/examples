{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Code Search on Kubeflow\n",
    "\n",
    "This notebook implements an end-to-end Semantic Code Search on top of [Kubeflow](https://www.kubeflow.org/) - given an input query string, get a list of code snippets semantically similar to the query string.\n",
    "\n",
    "**NOTE**: If you haven't already, see [kubeflow/examples/code_search](https://github.com/kubeflow/examples/tree/master/code_search) for instructions on how to get this notebook,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "Let us install all the Python dependencies. Note that everything must be done with `Python 2`. This will take a while and only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip2 install https://github.com/kubeflow/batch-predict/tarball/master\n",
    "\n",
    "! pip2 install -r src/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for BigQuery cells\n",
    "! pip2 install pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io import gbq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Variables\n",
    "\n",
    "This involves setting up the Ksonnet application as well as utility environment variables for various CLI steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables. Modify as desired.\n",
    "\n",
    "PROJECT = 'kubeflow-dev'\n",
    "CLUSTER_NAME = 'kubeflow-latest'\n",
    "CLUSTER_REGION = 'us-east1-d'\n",
    "CLUSTER_NAMESPACE = 'kubeflow-latest'\n",
    "\n",
    "TARGET_DATASET = 'code_search'\n",
    "WORKING_DIR = 'gs://kubeflow-examples/t2t-code-search/notebook-demo'\n",
    "WORKER_MACHINE_TYPE = 'n1-highcpu-32'\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# DO NOT MODIFY. These are environment variables to be used in a bash shell.\n",
    "%env PROJECT $PROJECT\n",
    "%env CLUSTER_NAME $CLUSTER_NAME\n",
    "%env CLUSTER_REGION $CLUSTER_REGION\n",
    "%env CLUSTER_NAMESPACE $CLUSTER_NAMESPACE\n",
    "\n",
    "%env TARGET_DATASET $TARGET_DATASET\n",
    "%env WORKING_DIR $WORKING_DIR\n",
    "%env WORKER_MACHINE_TYPE $WORKER_MACHINE_TYPE\n",
    "%env NUM_WORKERS $NUM_WORKERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Setup Authorization\n",
    "\n",
    "In a Kubeflow cluster, we already have the key credentials available with each pod and will re-use them to authenticate. This will allow us to submit `TFJob`s and execute `Dataflow` pipelines. We also set the new context for the Code Search Ksonnet application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Activate Service Account provided by Kubeflow.\n",
    "gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "\n",
    "# Get KUBECONFIG for the desired cluster.\n",
    "gcloud container clusters get-credentials ${CLUSTER_NAME} --region ${CLUSTER_REGION}\n",
    "\n",
    "# Set the namespace of the context.\n",
    "kubectl config set contexts.$(kubectl config current-context).namespace ${CLUSTER_NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ksonnet Application\n",
    "\n",
    "This will use the context we've set above and provide it as a new environment to the Ksonnet application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd kubeflow\n",
    "\n",
    "# Update Ksonnet application to the context set earlier\n",
    "ks env add code-search --context=$(kubectl config current-context)\n",
    "\n",
    "# Update the Working Directory of the application\n",
    "ks param set t2t-code-search workingDir ${WORKING_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Version Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Pip Version Info: \" && pip2 --version && echo\n",
    "echo \"Google Cloud SDK Info: \" && gcloud --version && echo\n",
    "echo \"Ksonnet Version Info: \" && ks version && echo\n",
    "echo \"Kubectl Version Info: \" && kubectl version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Github Files\n",
    "\n",
    "This is the query that is run as the first step of the Pre-Processing pipeline and is sent through a set of transformations. This is illustrative of the rows being processed in the pipeline we trigger next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "  SELECT\n",
    "    MAX(CONCAT(f.repo_name, ' ', f.path)) AS repo_path,\n",
    "    c.content\n",
    "  FROM\n",
    "    `bigquery-public-data.github_repos.files` AS f\n",
    "  JOIN\n",
    "    `bigquery-public-data.github_repos.contents` AS c\n",
    "  ON\n",
    "    f.id = c.id\n",
    "  JOIN (\n",
    "      --this part of the query makes sure repo is watched at least twice since 2017\n",
    "    SELECT\n",
    "      repo\n",
    "    FROM (\n",
    "      SELECT\n",
    "        repo.name AS repo\n",
    "      FROM\n",
    "        `githubarchive.year.2017`\n",
    "      WHERE\n",
    "        type=\"WatchEvent\"\n",
    "      UNION ALL\n",
    "      SELECT\n",
    "        repo.name AS repo\n",
    "      FROM\n",
    "        `githubarchive.month.2018*`\n",
    "      WHERE\n",
    "        type=\"WatchEvent\" )\n",
    "    GROUP BY\n",
    "      1\n",
    "    HAVING\n",
    "      COUNT(*) >= 2 ) AS r\n",
    "  ON\n",
    "    f.repo_name = r.repo\n",
    "  WHERE\n",
    "    f.path LIKE '%.py' AND --with python extension\n",
    "    c.size < 15000 AND --get rid of ridiculously long files\n",
    "    REGEXP_CONTAINS(c.content, r'def ') --contains function definition\n",
    "  GROUP BY\n",
    "    c.content\n",
    "  LIMIT\n",
    "    10\n",
    "\"\"\"\n",
    "\n",
    "gbq.read_gbq(query, dialect='standard', project_id=PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Github Files\n",
    "\n",
    "In this step, we will run a [Google Cloud Dataflow](https://cloud.google.com/dataflow/) pipeline (based on Apache Beam). A `Python 2` module `code_search.dataflow.cli.preprocess_github_dataset` has been provided which builds an Apache Beam pipeline. A list of all possible arguments can be seen via the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd src\n",
    "\n",
    "python2 -m code_search.dataflow.cli.preprocess_github_dataset -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Dataflow Job for Pre-Processing\n",
    "\n",
    "See help above for a short description of each argument. The values are being taken from environment variables defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd src\n",
    "\n",
    "JOB_NAME=\"preprocess-github-dataset-$(date +'%Y%m%d-%H%M%S')\"\n",
    "\n",
    "python2 -m code_search.dataflow.cli.preprocess_github_dataset \\\n",
    "        --runner DataflowRunner \\\n",
    "        --project \"${PROJECT}\" \\\n",
    "        --target_dataset \"${TARGET_DATASET}\" \\\n",
    "        --data_dir \"${WORKING_DIR}/data\" \\\n",
    "        --job_name \"${JOB_NAME}\" \\\n",
    "        --temp_location \"${WORKING_DIR}/dataflow/temp\" \\\n",
    "        --staging_location \"${WORKING_DIR}/dataflow/staging\" \\\n",
    "        --worker_machine_type \"${WORKER_MACHINE_TYPE}\" \\\n",
    "        --num_workers \"${NUM_WORKERS}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completed successfully, this should create a dataset in `BigQuery` named `target_dataset`. Additionally, it also dumps CSV files into `data_dir` which contain training samples (pairs of function and docstrings) for our Tensorflow Model. A representative set of results can be viewed using the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "  SELECT * \n",
    "  FROM \n",
    "    {}.token_pairs\n",
    "  LIMIT\n",
    "    10\n",
    "\"\"\".format(TARGET_DATASET)\n",
    "\n",
    "gbq.read_gbq(query, dialect='standard', project_id=PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Training\n",
    "\n",
    "In this step we will use `t2t-datagen` to convert the transformed data above into the `TFRecord` format. We will run this job on the Kubeflow cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd kubeflow\n",
    "\n",
    "ks apply code-search -c t2t-code-search-datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Tensorflow Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd kubeflow\n",
    "\n",
    "ks apply code-search -c t2t-code-search-trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd kubeflow\n",
    "\n",
    "ks apply code-search -c t2t-code-search-exporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Function Embeddings\n",
    "\n",
    "In this step, we will use the exported model above to compute function embeddings via another `Dataflow` pipeline. A `Python 2` module `code_search.dataflow.cli.create_function_embeddings` has been provided for this purpose. A list of all possible arguments can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd src\n",
    "\n",
    "python2 -m code_search.dataflow.cli.create_function_embeddings -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "First, select a Exported Model version from the `${WORKING_DIR}/output/export/Servo`. This should be name of a folder with UNIX Seconds Timestamp like `1533685294`. Below, we automatically do that by selecting the folder which represents the latest timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out EXPORT_DIR_LS\n",
    "\n",
    "gsutil ls ${WORKING_DIR}/output/export/Servo | grep -oE \"([0-9]+)/$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This routine will fail if no export has been completed successfully.\n",
    "MODEL_VERSION = max([int(ts[:-1]) for ts in EXPORT_DIR_LS.split('\\n') if ts])\n",
    "\n",
    "# DO NOT MODIFY. These are environment variables to be used in a bash shell.\n",
    "%env MODEL_VERSION $MODEL_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Dataflow Job for Function Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd src\n",
    "\n",
    "python2 -m code_search.dataflow.cli.create_function_embeddings \\\n",
    "        --runner DataflowRunner\n",
    "        --project \"${PROJECT}\" \\\n",
    "        --target_dataset \"${TARGET_DATASET}\" \\\n",
    "        --problem github_function_docstring \\\n",
    "        --data_dir \"${WORKING_DIR}/data\" \\\n",
    "        --saved_model_dir \"${WORKING_DIR}/output/export/Servo/${MODEL_VERSION}\" \\\n",
    "        --job_name compute-function-embeddings\n",
    "        --temp_location \"${WORKING_DIR}/dataflow/temp\" \\\n",
    "        --staging_location \"${WORKING_DIR}/dataflow/staging\" \\\n",
    "        --worker_machine_type \"${WORKER_MACHINE_TYPE}\" \\\n",
    "        --num_workers \"${NUM_WORKERS}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completed successfully, this should create another table in the same `BigQuery` dataset which contains the function embeddings for each existing data sample available from the previous Dataflow Job. Additionally, it also dumps a CSV file containing metadata for each of the function and its embeddings. A representative query result is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "  SELECT * \n",
    "  FROM \n",
    "    {}.function_embeddings\n",
    "  LIMIT\n",
    "    10\n",
    "\"\"\".format(TARGET_DATASET)\n",
    "\n",
    "gbq.read_gbq(query, dialect='standard', project_id=PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Search Index\n",
    "\n",
    "We now create the Search Index from the computed embeddings so that during a query we can do a k-Nearest Neighbor search to give out semantically similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd kubeflow\n",
    "\n",
    "ks apply code-search -c search-index-creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CSV files generated from the previous step, this creates an index using [NMSLib](https://github.com/nmslib/nmslib). A unified CSV file containing all the code examples for a human-readable reverse lookup during the query, is also created in the `WORKING_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy an Inference Server\n",
    "\n",
    "We've seen offline inference during the computation of embeddings. For online inference, we deploy the exported Tensorflow model above using [Tensorflow Serving](https://www.tensorflow.org/serving/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd kubeflow\n",
    "\n",
    "ks apply code-search -c t2t-code-search-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Search UI\n",
    "\n",
    "We finally deploy the Search UI which allows the user to input arbitrary strings and see a list of results corresponding to semantically similar Python functions. This internally uses the inference server we just deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd kubeflow\n",
    "\n",
    "ks apply code-search -c search-index-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service should now be available at FQDN of the Kubeflow cluster at path `/code-search/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
