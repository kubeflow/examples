{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Code Search on Kubeflow\n",
    "\n",
    "This notebook implements an end-to-end Semantic Code Search on top of [Kubeflow](https://www.kubeflow.org/) - given an input query string, get a list of code snippets semantically similar to the query string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**NOTE**: If using the JupyterHub Spawner on a Kubeflow cluster, use the Docker image `gcr.io/kubeflow-images-public/kubeflow-codelab-notebook:v20180808-v0.2-22-gcfdcb12` which has baked all the pre-prequisites.\n",
    "\n",
    "* `Kubeflow v0.2.2`\n",
    "  This notebook assumes a Kubeflow cluster is already deployed. See [Getting Started with Kubeflow](https://www.kubeflow.org/docs/started/getting-started/).\n",
    "\n",
    "* `Python 2.7` (bundled with `pip`) \n",
    "  For this demo, we will use Python 2.7. This restriction is due to [Apache Beam](https://beam.apache.org/), which\n",
    "  does not support Python 3 yet (See [BEAM-1251](https://issues.apache.org/jira/browse/BEAM-1251)).\n",
    "\n",
    "* `Google Cloud SDK`\n",
    "  This example will use tools from the [Google Cloud SDK](https://cloud.google.com/sdk/). The SDK must be\n",
    "  authenticated and authorized. See [Authentication Overview](https://cloud.google.com/docs/authentication/).\n",
    "  \n",
    "* `Ksonnet 0.12`\n",
    "  We use [Ksonnet](https://ksonnet.io/) to write Kubernetes jobs in a declarative manner to be run on top of Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Pip Version Info: \" && pip2 --version && echo\n",
    "echo \"Google Cloud SDK Info: \" && gcloud --version && echo\n",
    "echo \"Ksonnet Version Info: \" && ks version && echo\n",
    "echo \"Kubectl Version Info: \" && kubectl version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Source Code\n",
    "\n",
    "Let us clone the source code for Code Search from [kubeflow/examples](https://github.com/kubeflow/examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "##\n",
    "# NOTE: This cell must only be run once or the clone will complain.\n",
    "# Only if necessary, uncomment the line below.\n",
    "\n",
    "# rm -rf examples\n",
    "git clone --depth=1 https://github.com/kubeflow/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also change the working directory to the root directory of our Code Search example. This will allow us to perform all downstream operations without long winded paths and relative to the `code_search` project source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('examples/code_search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "Let us install all the Python dependencies. Note that everything must be done with `Python 2`. This should take a while.\n",
    "\n",
    "**NOTE**: The Kubeflow Batch Prediction dependency is installed from a fork for reasons in [kubeflow/batch-preidct#9](https://github.com/kubeflow/batch-predict/pull/9) and corresponding issue [kubeflow/batch-preidct#10](https://github.com/kubeflow/batch-predict/pull/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip2 install -r src/requirements.txt\n",
    "pip2 install https://github.com/activatedgeek/batch-predict/tarball/fix-value-provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Variables\n",
    "\n",
    "This involves setting up the Ksonnet application as well as utility environment variables for various CLI steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=kubeflow-dev\n",
      "env: CLUSTER_NAME=kubeflow-latest\n",
      "env: CLUSTER_REGION=us-central1-a\n",
      "env: TARGET_DATASET=code_search\n",
      "env: WORKING_DIR=gs://kubeflow-examples/t2t-code-search/20180809\n",
      "env: WORKER_MACHINE_TYPE=n1-highcpu-32\n",
      "env: NUM_WORKERS=16\n"
     ]
    }
   ],
   "source": [
    "# Configuration Variables. Modify as desired.\n",
    "\n",
    "PROJECT = 'kubeflow-dev'\n",
    "CLUSTER_NAME = 'kubeflow-latest'\n",
    "CLUSTER_REGION = 'us-central1-a'\n",
    "CLUSTER_NAMESPACE = 'kubeflow-latest'\n",
    "\n",
    "TARGET_DATASET = 'code_search'\n",
    "WORKING_DIR = 'gs://kubeflow-examples/t2t-code-search/20180809'\n",
    "WORKER_MACHINE_TYPE = 'n1-highcpu-32'\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# DO NOT MODIFY. The following statements are needed to support\n",
    "# environment variables in a bash cell.\n",
    "%env PROJECT $PROJECT\n",
    "%env CLUSTER_NAME $CLUSTER_NAME\n",
    "%env CLUSTER_REGION $CLUSTER_REGION\n",
    "\n",
    "%env TARGET_DATASET $TARGET_DATASET\n",
    "%env WORKING_DIR $WORKING_DIR\n",
    "%env WORKER_MACHINE_TYPE $WORKER_MACHINE_TYPE\n",
    "%env NUM_WORKERS $NUM_WORKERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the Kubernetes cluster credentials and the Ksonnet application (present in the `examples/code_search/kubeflow` directory) and update parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"service_account\",\n",
      "  \"project_id\": \"kubeflow-dev\",\n",
      "  \"private_key_id\": \"7fa99e3b3c8803b32beb6f2b0a147410bc806eec\",\n",
      "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC1OUvIB9hWVCQu\\nJ/MdU7XH87sA4/iPT3cKjirIxTgT7kA5jXNnpPet0G4OhpjRje9Em+oIpdMcg8sX\\nv8OuluagUWi8UsU0naP0v8y8Uc2onYtaC59nCn+vg9EAyCutpT2wJeZH6mDH2Kta\\nubDh6NcobWqwCfqNS7tF7DYaxl/EQ3mvAt+TlAbPnSjRP7POIEE4DzOl3QuIZ9AN\\nbWiPn0Y/2+cpRkPqDIRAKDTBLAGcSIF//7aMSSalPh15dyGgdKPvF/SMyl+xiXQ9\\nA4p4k7UPzZcUpWiOvAwIkncs0Q1VnYuIuAvjj/u3ufFREInAlqz4ECZo0GQbbIaF\\n3U8P448ZAgMBAAECggEAQs8XHWyq+BR37B4lNcQVCVxUrfzdNvP8NkN4CWEPjeVw\\n/uajS2vZNVZYJHnBX8u8ECaMjliXrfT2S9CR0szlw+ePPZIkCoQtG/8Ter+LmmRO\\nKcmMH+ASd4GYbPnehFsdFVG7hfqlaDd74GwBhh8hJtHDmZdsK2fmZ94vigpk5sS8\\ndrdmSXRyrh8epvV54z/zZ6gIE3HrBbZD1nGJg81TOPtPQBwub+MCtm6Wg6sUsFdc\\nxDvC3ExIM3Nqyrrw71dbTNY/MVCZRQYSgQolcJraAFQuTPFmHrgtKeEXRV5/jAl+\\nhEDwf7o5U2NcnAD8Yq7VHMFyQP38T3fEKhEo7DlCGwKBgQDd8pmB2oDE/RFOmpqw\\nb+xJJQd/GRx+mtb3SVKm19sLAt+ANYDJMkChPSFbBBDkV3k4/K9Euq0Ahx22e9Y7\\nCyWz8IRLKGYd2e+HHtGVLAEtBRZXoTyCcyakoGncRx7YxOy6eWkn+cRZzO0Gxvfy\\nl1a2MGrusHoJE9E6/R/EqSETHwKBgQDRBzFkyrmwd+rMbBPv3oqHpB6BRuY8HiDu\\n7MnXnv7HPMS1SFeyGf0DXz0nlVgZmnyig0FzVrJz/TWeKrYRw3HYB6w9BIB1ALQ+\\n9XKHffpArVwJq6UXzQM2/nksitbqV1iCt5HuvYuERjQEyWOcJmRYy6JTLKkuB6He\\nTGYtpZ4OxwKBgEwy0gkG31bCG4MFCT6x7klecShtkp7UwiC8B6hIS2eIYPs/Yyuw\\nGQhCWIVK7BG5BmFOP041WNfpi4XvFinHNfaFCKZVBFoTTGzzY7j3FqBCMt16+a8/\\noXC7shxyPmDlmvCaZkPXOFYsSOQew1mCI78A+HViGUOsjhTWemWmOzi5AoGAQEB2\\nxWWvWHgz+2xJYJVGfdVL7y8M/HPCac7taFMeO74JNTQsiLmGVXAEC1PTxeL/5rB0\\nAPEoX5D410qHtsQeDWqmXgcxOUXjIesurkliEFl5jhtt3vgMwe3M740p7NS2h0/I\\nEtu+tAUkXkhNTyVZ+RV3KxvyUQzVi9BuJB/hiucCgYEAuSyOS3fhZASLbhi1DbsB\\nxzAQkGSwHA7TDdr7jxOD1LLetvN6nfjBj/KjsUg1C99K8R61EbZyH3+jLAkyzskj\\nu8M0ozPOyw18rxTWy6pgTPRJ+5XiD96Tf8BDoI1xTodGJGHzqzHEfOx0sMVh95QX\\nz0U0nta7qbBELSs6MgyTmjo=\\n-----END PRIVATE KEY-----\\n\",\n",
      "  \"client_email\": \"kubeflow-latest-user@kubeflow-dev.iam.gserviceaccount.com\",\n",
      "  \"client_id\": \"112957758415637004516\",\n",
      "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
      "  \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n",
      "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
      "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/kubeflow-latest-user%40kubeflow-dev.iam.gserviceaccount.com\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# FIXME: The deployment must set up the service account with GKE permissions\n",
    "# gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "\n",
    "# gcloud container clusters get-credentials ${CLUSTER_NAME} --region ${CLUSTER_REGION}\n",
    "# kubectl config view\n",
    "# ks env add code-search --context=$(kubectl config current-context) --namespace=kubeflow\n",
    "cat ${GOOGLE_APPLICATION_CREDENTIALS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Github Files\n",
    "\n",
    "In this step, we will run a [Google Cloud Dataflow](https://cloud.google.com/dataflow/) pipeline (based on Apache Beam). A `Python 2` module `code_search.dataflow.cli.preprocess_github_dataset` has been provided which builds an Apache Beam pipeline. A list of all possible arguments can be seen via the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python2 -m code_search.dataflow.cli.preprocess_github_dataset -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "We use a subset of the options available to run our Dataflow job. The variables are required and make sure you modify the preset variables as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Dataflow Job for Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# See help for a short description of each argument.\n",
    "\n",
    "python2 -m code_search.dataflow.cli.preprocess_github_dataset \\\n",
    "        --runner DataflowRunner \\\n",
    "        --project \"${PROJECT}\" \\\n",
    "        --target_dataset \"${TARGET_DATASET}\" \\\n",
    "        --data_dir \"${WORKING_DIR}/data\" \\\n",
    "        --job_name \"preprocess-github-dataset\" \\\n",
    "        --temp_location \"${WORKING_DIR}/data/dataflow/temp\" \\\n",
    "        --staging_location \"${WORKING_DIR}/data/dataflow/staging\" \\\n",
    "        --worker_machine_type \"${WORKER_MACHINE_TYPE}\" \\\n",
    "        --num_workers \"${NUM_WORKERS}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completed successfully, this should create a dataset in `BigQuery` named `target_dataset`. Additionally, it also dumps CSV files into `data_dir` which contain training samples (pairs of function and docstrings) for our Tensorflow Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Training\n",
    "\n",
    "In this step we will use `t2t-datagen` to convert the transformed data above into the `TFRecord` format. We will run this job on the Kubeflow cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c t2t-code-search-datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Tensorflow Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c t2t-code-search-trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c t2t-code-search-exporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Function Embeddings\n",
    "\n",
    "In this step, we will use the exported model above to compute function embeddings via another `Dataflow` pipeline. A `Python 2` module `code_search.dataflow.cli.create_function_embeddings` has been provided for this purpose. A list of all possible arguments can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python2 -m code_search.dataflow.cli.create_function_embeddings -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "First, select a Exported Model version from the `${WORKING_DIR}/output/export/Servo`. This should be name of a folder with UNIX Seconds Timestamp like `1533685294`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = '1533685294'\n",
    "\n",
    "# DO NOT MODIFY. The following statements are needed to support\n",
    "# environment variables in a bash cell.\n",
    "%env MODEL_VERSION $MODEL_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Dataflow Job for Function Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python2 -m code_search.dataflow.cli.create_function_embeddings \\\n",
    "        --runner DataflowRunner\n",
    "        --project \"${PROJECT}\" \\\n",
    "        --target_dataset \"${TARGET_DATASET}\" \\\n",
    "        --problem github_function_docstring \\\n",
    "        --data_dir \"${WORKING_DIR}/data\" \\\n",
    "        --saved_model_dir \"${WORKING_DIR}/output/export/Servo/${MODEL_VERSION}\" \\\n",
    "        --job_name compute-function-embeddings\n",
    "        --temp_location \"${WORKING_DIR}/data/dataflow/temp\" \\\n",
    "        --staging_location \"${WORKING_DIR}/data/dataflow/staging\" \\\n",
    "        --worker_machine_type \"${WORKER_MACHINE_TYPE}\" \\\n",
    "        --num_workers \"${NUM_WORKERS}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completed successfully, this should create another table in the same `BigQuery` dataset which contains the function embeddings for each existing data sample available from the previous Dataflow Job. Additionally, it also dumps a CSV file containing metadata for each of the function and its embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Search Index\n",
    "\n",
    "We now create the Search Index from the computed embeddings so that during a query we can do a k-Nearest Neighbor search to give out semantically similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c search-index-creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CSV files generated from the previous step, this creates an index using [NMSLib](https://github.com/nmslib/nmslib). A unified CSV file containing all the code examples for a human-readable reverse lookup during the query, is also created in the `WORKING_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy an Inference Server\n",
    "\n",
    "We've seen offline inference during the computation of embeddings. For online inference, we deploy the exported Tensorflow model above using [Tensorflow Serving](https://www.tensorflow.org/serving/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c t2t-code-search-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Search UI\n",
    "\n",
    "We finally deploy the Search UI which allows the user to input arbitrary strings and see a list of results corresponding to semantically similar Python functions. This internally uses the inference server we just deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c search-index-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service should now be available at FQDN of the Kubeflow cluster at path `/code-search/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
