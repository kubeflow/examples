{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Code Search on Kubeflow\n",
    "\n",
    "This notebook implements an end-to-end Semantic Code Search on top of [Kubeflow](https://www.kubeflow.org/) - given an input query string, get a list of code snippets semantically similar to the query string.\n",
    "\n",
    "**NOTE**: For instructions on how to get this notebook, see [kubeflow/examples/code_search](https://github.com/kubeflow/examples/tree/master/code_search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "Let us install all the Python dependencies. Note that everything must be done with `Python 2`. This will take a while and only needs to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME(sanyamkapoor): The Kubeflow Batch Prediction dependency is installed from a fork for reasons in\n",
    "# kubeflow/batch-predict#9 and corresponding issue kubeflow/batch-predict#10\n",
    "! pip2 install https://github.com/activatedgeek/batch-predict/tarball/fix-value-provider\n",
    "\n",
    "! pip2 install -r src/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version  \r\n",
      "------------------------ ---------\r\n",
      "absl-py                  0.3.0    \r\n",
      "alembic                  1.0.0    \r\n",
      "annoy                    1.12.0   \r\n",
      "asn1crypto               0.24.0   \r\n",
      "astor                    0.7.1    \r\n",
      "async-generator          1.10     \r\n",
      "backcall                 0.1.0    \r\n",
      "bleach                   1.5.0    \r\n",
      "bz2file                  0.98     \r\n",
      "cachetools               2.1.0    \r\n",
      "certifi                  2018.4.16\r\n",
      "cffi                     1.11.4   \r\n",
      "chardet                  3.0.4    \r\n",
      "click                    6.7      \r\n",
      "conda                    4.5.9    \r\n",
      "cryptography             2.1.4    \r\n",
      "cycler                   0.10.0   \r\n",
      "cymem                    1.31.2   \r\n",
      "cytoolz                  0.9.0.1  \r\n",
      "dask                     0.18.2   \r\n",
      "decorator                4.3.0    \r\n",
      "dill                     0.2.8.2  \r\n",
      "en-core-web-sm           2.0.0    \r\n",
      "entrypoints              0.2.3    \r\n",
      "Flask                    1.0.2    \r\n",
      "ftfy                     4.4.3    \r\n",
      "future                   0.16.0   \r\n",
      "gast                     0.2.0    \r\n",
      "gevent                   1.3.5    \r\n",
      "google-api-core          1.3.0    \r\n",
      "google-api-python-client 1.7.4    \r\n",
      "google-auth              1.5.1    \r\n",
      "google-auth-httplib2     0.0.3    \r\n",
      "google-auth-oauthlib     0.2.0    \r\n",
      "google-cloud             0.34.0   \r\n",
      "google-cloud-bigquery    1.5.0    \r\n",
      "google-cloud-core        0.28.1   \r\n",
      "google-resumable-media   0.3.1    \r\n",
      "googleapis-common-protos 1.5.3    \r\n",
      "greenlet                 0.4.14   \r\n",
      "grpcio                   1.14.0   \r\n",
      "gunicorn                 19.9.0   \r\n",
      "gym                      0.10.5   \r\n",
      "h5py                     2.8.0    \r\n",
      "html5lib                 0.9999999\r\n",
      "httplib2                 0.11.3   \r\n",
      "idna                     2.6      \r\n",
      "ijson                    2.3      \r\n",
      "ipykernel                4.8.2    \r\n",
      "ipython                  6.5.0    \r\n",
      "ipython-genutils         0.2.0    \r\n",
      "ipywidgets               7.3.2    \r\n",
      "itsdangerous             0.24     \r\n",
      "jedi                     0.12.1   \r\n",
      "Jinja2                   2.10     \r\n",
      "jsonschema               2.6.0    \r\n",
      "jupyter                  1.0.0    \r\n",
      "jupyter-client           5.2.3    \r\n",
      "jupyter-console          5.2.0    \r\n",
      "jupyter-core             4.4.0    \r\n",
      "jupyterhub               0.9.1    \r\n",
      "jupyterlab               0.33.6   \r\n",
      "jupyterlab-launcher      0.11.2   \r\n",
      "Keras                    2.2.2    \r\n",
      "Keras-Applications       1.0.4    \r\n",
      "Keras-Preprocessing      1.0.2    \r\n",
      "kiwisolver               1.0.1    \r\n",
      "ktext                    0.33     \r\n",
      "Mako                     1.0.7    \r\n",
      "Markdown                 2.6.11   \r\n",
      "MarkupSafe               1.0      \r\n",
      "matplotlib               2.2.2    \r\n",
      "mistune                  0.8.3    \r\n",
      "more-itertools           4.3.0    \r\n",
      "mpmath                   1.0.0    \r\n",
      "msgpack                  0.5.6    \r\n",
      "msgpack-numpy            0.4.3.1  \r\n",
      "multiprocess             0.70.6.1 \r\n",
      "murmurhash               0.28.0   \r\n",
      "nbconvert                5.3.1    \r\n",
      "nbformat                 4.4.0    \r\n",
      "networkx                 2.1      \r\n",
      "nltk                     3.3      \r\n",
      "notebook                 5.6.0    \r\n",
      "numpy                    1.15.0   \r\n",
      "oauth2client             4.1.2    \r\n",
      "oauthlib                 2.1.0    \r\n",
      "pamela                   0.3.0    \r\n",
      "pandas                   0.23.3   \r\n",
      "pandas-gbq               0.5.0    \r\n",
      "pandocfilters            1.4.2    \r\n",
      "parso                    0.3.1    \r\n",
      "pathos                   0.2.2.1  \r\n",
      "pexpect                  4.6.0    \r\n",
      "pickleshare              0.7.4    \r\n",
      "Pillow                   5.2.0    \r\n",
      "pip                      18.0     \r\n",
      "plac                     0.9.6    \r\n",
      "pox                      0.2.4    \r\n",
      "ppft                     1.6.4.8  \r\n",
      "preshed                  1.0.1    \r\n",
      "prometheus-client        0.3.1    \r\n",
      "prompt-toolkit           1.0.15   \r\n",
      "protobuf                 3.6.0    \r\n",
      "ptyprocess               0.6.0    \r\n",
      "pyarrow                  0.10.0   \r\n",
      "pyasn1                   0.4.4    \r\n",
      "pyasn1-modules           0.2.2    \r\n",
      "pycosat                  0.6.3    \r\n",
      "pycparser                2.18     \r\n",
      "pydot                    1.2.4    \r\n",
      "pyemd                    0.5.1    \r\n",
      "pyglet                   1.3.2    \r\n",
      "Pygments                 2.2.0    \r\n",
      "pyOpenSSL                17.5.0   \r\n",
      "pyparsing                2.2.0    \r\n",
      "Pyphen                   0.9.4    \r\n",
      "PySocks                  1.6.7    \r\n",
      "python-dateutil          2.7.3    \r\n",
      "python-editor            1.0.3    \r\n",
      "python-Levenshtein       0.12.0   \r\n",
      "python-oauth2            1.1.0    \r\n",
      "pytz                     2018.5   \r\n",
      "PyYAML                   3.13     \r\n",
      "pyzmq                    17.1.0   \r\n",
      "qtconsole                4.3.1    \r\n",
      "regex                    2017.4.5 \r\n",
      "requests                 2.18.4   \r\n",
      "requests-oauthlib        1.0.0    \r\n",
      "rsa                      3.4.2    \r\n",
      "ruamel-yaml              0.15.35  \r\n",
      "scikit-learn             0.19.2   \r\n",
      "scipy                    1.1.0    \r\n",
      "Send2Trash               1.5.0    \r\n",
      "setuptools               38.4.0   \r\n",
      "simplegeneric            0.8.1    \r\n",
      "six                      1.11.0   \r\n",
      "sklearn                  0.0      \r\n",
      "spacy                    2.0.12   \r\n",
      "SQLAlchemy               1.2.10   \r\n",
      "sympy                    1.2      \r\n",
      "tensor2tensor            1.6.6    \r\n",
      "tensorboard              1.8.0    \r\n",
      "tensorflow               1.8.0    \r\n",
      "termcolor                1.1.0    \r\n",
      "terminado                0.8.1    \r\n",
      "testpath                 0.3.1    \r\n",
      "textacy                  0.6.2    \r\n",
      "thinc                    6.10.3   \r\n",
      "toolz                    0.9.0    \r\n",
      "tornado                  5.1      \r\n",
      "tqdm                     4.24.0   \r\n",
      "traitlets                4.3.2    \r\n",
      "ujson                    1.35     \r\n",
      "Unidecode                1.0.22   \r\n",
      "uritemplate              3.0.0    \r\n",
      "urllib3                  1.22     \r\n",
      "wcwidth                  0.1.7    \r\n",
      "Werkzeug                 0.14.1   \r\n",
      "wheel                    0.30.0   \r\n",
      "widgetsnbextension       3.3.1    \r\n",
      "wrapt                    1.10.11  \r\n"
     ]
    }
   ],
   "source": [
    "# Only for BigQuery cells\n",
    "\n",
    "! pip2 install datalab\n",
    "\n",
    "%load_ext google.datalab.kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Variables\n",
    "\n",
    "This involves setting up the Ksonnet application as well as utility environment variables for various CLI steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Variables. Modify as desired.\n",
    "\n",
    "PROJECT = 'kubeflow-dev'\n",
    "CLUSTER_NAME = 'kubeflow-latest'\n",
    "CLUSTER_REGION = 'us-east1-d'\n",
    "CLUSTER_NAMESPACE = 'kubeflow-latest'\n",
    "\n",
    "TARGET_DATASET = 'code_search'\n",
    "WORKING_DIR = 'gs://kubeflow-examples/t2t-code-search/20180813'\n",
    "WORKER_MACHINE_TYPE = 'n1-highcpu-32'\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# DO NOT MODIFY. These are environment variables to be used in a\n",
    "# bash shell.\n",
    "%env PROJECT $PROJECT\n",
    "%env CLUSTER_NAME $CLUSTER_NAME\n",
    "%env CLUSTER_REGION $CLUSTER_REGION\n",
    "%env CLUSTER_NAMESPACE $CLUSTER_NAMESPACE\n",
    "\n",
    "%env TARGET_DATASET $TARGET_DATASET\n",
    "%env WORKING_DIR $WORKING_DIR\n",
    "%env WORKER_MACHINE_TYPE $WORKER_MACHINE_TYPE\n",
    "%env NUM_WORKERS $NUM_WORKERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Setup Authorization\n",
    "\n",
    "In a Kubeflow cluster, we already have the key credentials available with each pod and will re-use them to authenticate. This will allow us to submit `TFJob`s and execute `Dataflow` pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# FIXME: The deployment must set up the service account with GKE permissions\n",
    "gcloud auth activate-service-account --key-file=${GOOGLE_APPLICATION_CREDENTIALS}\n",
    "gcloud container clusters get-credentials ${CLUSTER_NAME} --region ${CLUSTER_REGION}\n",
    "kubectl config set contexts.$(kubectl config current-context).namespace ${CLUSTER_NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Version Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Pip Version Info: \" && pip2 --version && echo\n",
    "echo \"Google Cloud SDK Info: \" && gcloud --version && echo\n",
    "echo \"Ksonnet Version Info: \" && ks version && echo\n",
    "echo \"Kubectl Version Info: \" && kubectl version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Github Files\n",
    "\n",
    "This is the query that is run as the first step of the Pre-Processing pipeline and is sent through a set of transformations. This is illustrative of the rows being processed in the pipeline we trigger next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT\n",
    "  MAX(CONCAT(f.repo_name, ' ', f.path)) AS repo_path,\n",
    "  c.content\n",
    "FROM\n",
    "  `bigquery-public-data.github_repos.files` AS f\n",
    "JOIN\n",
    "  `bigquery-public-data.github_repos.contents` AS c\n",
    "ON\n",
    "  f.id = c.id\n",
    "JOIN (\n",
    "    --this part of the query makes sure repo is watched at least twice since 2017\n",
    "  SELECT\n",
    "    repo\n",
    "  FROM (\n",
    "    SELECT\n",
    "      repo.name AS repo\n",
    "    FROM\n",
    "      `githubarchive.year.2017`\n",
    "    WHERE\n",
    "      type=\"WatchEvent\"\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "      repo.name AS repo\n",
    "    FROM\n",
    "      `githubarchive.month.2018*`\n",
    "    WHERE\n",
    "      type=\"WatchEvent\" )\n",
    "  GROUP BY\n",
    "    1\n",
    "  HAVING\n",
    "    COUNT(*) >= 2 ) AS r\n",
    "ON\n",
    "  f.repo_name = r.repo\n",
    "WHERE\n",
    "  f.path LIKE '%.py' AND --with python extension\n",
    "  c.size < 15000 AND --get rid of ridiculously long files\n",
    "  REGEXP_CONTAINS(c.content, r'def ') --contains function definition\n",
    "GROUP BY\n",
    "  c.content\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Github Files\n",
    "\n",
    "In this step, we will run a [Google Cloud Dataflow](https://cloud.google.com/dataflow/) pipeline (based on Apache Beam). A `Python 2` module `code_search.dataflow.cli.preprocess_github_dataset` has been provided which builds an Apache Beam pipeline. A list of all possible arguments can be seen via the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd src\n",
    "\n",
    "python2 -m code_search.dataflow.cli.preprocess_github_dataset -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Dataflow Job for Pre-Processing\n",
    "\n",
    "See help above for a short description of each argument. The values are being taken from environment variables defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Move\n",
    "cd src\n",
    "\n",
    "JOB_NAME=\"preprocess-github-dataset-$(date +'%Y%m%d-%H%M%S')\"\n",
    "\n",
    "python2 -m code_search.dataflow.cli.preprocess_github_dataset \\\n",
    "        --runner DataflowRunner \\\n",
    "        --project \"${PROJECT}\" \\\n",
    "        --target_dataset \"${TARGET_DATASET}\" \\\n",
    "        --data_dir \"${WORKING_DIR}/data\" \\\n",
    "        --job_name \"${JOB_NAME}\" \\\n",
    "        --temp_location \"${WORKING_DIR}/data/dataflow/temp\" \\\n",
    "        --staging_location \"${WORKING_DIR}/data/dataflow/staging\" \\\n",
    "        --worker_machine_type \"${WORKER_MACHINE_TYPE}\" \\\n",
    "        --num_workers \"${NUM_WORKERS}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completed successfully, this should create a dataset in `BigQuery` named `target_dataset`. Additionally, it also dumps CSV files into `data_dir` which contain training samples (pairs of function and docstrings) for our Tensorflow Model. A representative set of results can be viewed using the following query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT * FROM $PROJECT.code_search.token_pairs LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset for Training\n",
    "\n",
    "In this step we will use `t2t-datagen` to convert the transformed data above into the `TFRecord` format. We will run this job on the Kubeflow cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c t2t-code-search-datagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Tensorflow Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c t2t-code-search-trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c t2t-code-search-exporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Function Embeddings\n",
    "\n",
    "In this step, we will use the exported model above to compute function embeddings via another `Dataflow` pipeline. A `Python 2` module `code_search.dataflow.cli.create_function_embeddings` has been provided for this purpose. A list of all possible arguments can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python2 -m code_search.dataflow.cli.create_function_embeddings -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "First, select a Exported Model version from the `${WORKING_DIR}/output/export/Servo`. This should be name of a folder with UNIX Seconds Timestamp like `1533685294`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_VERSION = '1533685294'\n",
    "\n",
    "# DO NOT MODIFY. The following statements are needed to support\n",
    "# environment variables in a bash cell.\n",
    "%env MODEL_VERSION $MODEL_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Dataflow Job for Function Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python2 -m code_search.dataflow.cli.create_function_embeddings \\\n",
    "        --runner DataflowRunner\n",
    "        --project \"${PROJECT}\" \\\n",
    "        --target_dataset \"${TARGET_DATASET}\" \\\n",
    "        --problem github_function_docstring \\\n",
    "        --data_dir \"${WORKING_DIR}/data\" \\\n",
    "        --saved_model_dir \"${WORKING_DIR}/output/export/Servo/${MODEL_VERSION}\" \\\n",
    "        --job_name compute-function-embeddings\n",
    "        --temp_location \"${WORKING_DIR}/data/dataflow/temp\" \\\n",
    "        --staging_location \"${WORKING_DIR}/data/dataflow/staging\" \\\n",
    "        --worker_machine_type \"${WORKER_MACHINE_TYPE}\" \\\n",
    "        --num_workers \"${NUM_WORKERS}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When completed successfully, this should create another table in the same `BigQuery` dataset which contains the function embeddings for each existing data sample available from the previous Dataflow Job. Additionally, it also dumps a CSV file containing metadata for each of the function and its embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Search Index\n",
    "\n",
    "We now create the Search Index from the computed embeddings so that during a query we can do a k-Nearest Neighbor search to give out semantically similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c search-index-creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the CSV files generated from the previous step, this creates an index using [NMSLib](https://github.com/nmslib/nmslib). A unified CSV file containing all the code examples for a human-readable reverse lookup during the query, is also created in the `WORKING_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy an Inference Server\n",
    "\n",
    "We've seen offline inference during the computation of embeddings. For online inference, we deploy the exported Tensorflow model above using [Tensorflow Serving](https://www.tensorflow.org/serving/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c t2t-code-search-serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Search UI\n",
    "\n",
    "We finally deploy the Search UI which allows the user to input arbitrary strings and see a list of results corresponding to semantically similar Python functions. This internally uses the inference server we just deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ks apply code-search -c search-index-server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service should now be available at FQDN of the Kubeflow cluster at path `/code-search/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
