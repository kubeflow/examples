{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Google LLC. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 storage of preprocessed Images for DNN/CNN\n",
    "\n",
    "This 'ML pipeline' module uses HDF5 file storage in Python to store preprocessed images for a Deep Neural Network (DNN) or Convoluntional Neural Network (CNN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Preprocessed Images to HDF5 storage.\n",
    "\n",
    "Depending on the resources (memory/CPU) during training, a dataset of preprocessed data (machine learning ready data) maybe too large to keep entirely in memory. \n",
    "\n",
    "There are several factors to consider 'how big will be the in-memory dataset when preprocessed into machine learning ready data?'. The in-memory dataset will be considerable larger than the on-disk storage image files (e.g., JPEG, PNG, etc), for the following reasons:\n",
    "\n",
    "        1. The on-disk image formats use (lossy or lossless) compression.\n",
    "        2. Pixel data in images is generally 8 bits per pixel, which is stored as a uint8 (single byte), while\n",
    "           the machine learning ready data version, the pixels are generally float32 (4 bytes).\n",
    "           \n",
    "As a general rule of thumb, the in-memory size of the preprocessed data will be approxiametly 8 times greater than the on-disk version stored in compressed image formats.\n",
    "\n",
    "*Best Practices*\n",
    "\n",
    "The HDF5 file format is optimized for fast access/retrieval of data, particularly when data elements are the same size and can be indexed in a continuously byte arrays, such as numpy arrays. In Python, numpy arrays are implemented in C as continuous byte arrays with a Python linkage, which gives the *near metal* high-performance access. \n",
    "\n",
    "The HDF5 file format is optimized for indexing and accessing these types of array structures (and corresponding metadata, i.e., attributes) and supports chunking and sharding.\n",
    "\n",
    "The HDF5 file format is currently the *work-horse* for storage/retrieval of machine learning ready data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def store_dataset(name, dataset, verbose=False):\n",
    "    \"\"\" Store a dataset of preprocessed image data to HDF5 file, where each HDF5 dataset is a collection of preprocessed\n",
    "        images with the same label (class). The name of the HDF5 dataset is the label.\n",
    "    Args:\n",
    "        name   : (str) name of the dataset to use as the basename of the HDF5 file\n",
    "        dataset: (list(tuple(numpy,list))) preprocessed dataset.\n",
    "        verbose: (bool) display (print to console) the collections and labels written to the HDF5 file.\n",
    "    Returns:\n",
    "        Nothing\n",
    "        \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Write the images and labels to disk as HD5 file\n",
    "    with h5py.File(name + '.h5', 'w') as hf:\n",
    "        for ix in range(0, len(dataset)):\n",
    "            try:\n",
    "                hf.create_dataset(dataset[ix][1], data=dataset[ix][0])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass # what to do ?\n",
    "\n",
    "    if verbose:\n",
    "        with h5py.File(name + '.h5') as hf:\n",
    "            print(list(hf.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_dataset(name):\n",
    "    \"\"\" Load a dataset (machine learning ready data) from a HDF5 file.\n",
    "    Args:\n",
    "        name   : (str) name of the dataset to use as the basename of the HDF5 file.\n",
    "\n",
    "    Return:\n",
    "        The dataset as a multi-dimensional numpy array, the corresponding labels as a numpy multi-dimensional array,\n",
    "        and the mapping of classes (names) to labels (integer values).\n",
    "        \n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    collections=[]\n",
    "    classes=[]\n",
    "    \n",
    "    # Read the images and labels from disk as HD5 file\n",
    "    with h5py.File(name + '.h5', 'r') as hf:\n",
    "        labels = list(hf.keys())\n",
    "        for label in labels:\n",
    "            collections.append( hf[label][:] )\n",
    "            classes.append(label)\n",
    "            \n",
    "    # expand labels in each class to equal number of images per class\n",
    "    new_labels = []\n",
    "    for i in range(len(labels)):\n",
    "        new_labels.append( np.full(len(collections[i]), i) )\n",
    "    labels = np.concatenate(new_labels)\n",
    "    \n",
    "    # merge the collections together into a dataset \n",
    "    collections = np.concatenate(collections)\n",
    "   \n",
    "    return collections, labels, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
