{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Google LLC. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Demonstration for Computer Vision / Color - Fruits360\n",
    "\n",
    "This demonstration will use the Frutis360 dataset. This is a dataset provided by Horea MuresÂ¸Faculty of Mathematics and Computer Science, Bolyai University, Romania and MiHai Oltean, Faculty of Exact Sciences and\n",
    "Engineering, University of Alba Iulia, Romania. The dataset consists of 81 classes of fruits and their varieties. The dataset was originally used for their research on their published paper *Fruit Recognition from images using Deep Learning*, 2018, https://arxiv.org/pdf/1712.00580.pdf . The dataset consists of 55,000. The images are split into a folder for training and a second folder for test (evaluation).\n",
    "\n",
    "Below is the description on how they prepared the images:\n",
    "\n",
    "    Fruits were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded.\n",
    "\n",
    "    A Logitech C920 camera was used for filming the fruits. This is one of the best webcams available.\n",
    "\n",
    "    Behind the fruits we placed a white sheet of paper as background.\n",
    "\n",
    "    However due to the variations in the lighting conditions, the background was not uniform and we wrote a dedicated algorithm which extract the fruit from the background. This algorithm is of flood fill type: we start from each edge of the image and we mark all pixels there, then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. We repeat the previous step until no more pixels can be marked.\n",
    "\n",
    "    All marked pixels are considered as being background (which is then filled with white) and the rest of pixels are considered as belonging to the object.\n",
    "\n",
    "    The maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequistes\n",
    "\n",
    "The following needs to be pre-installed:\n",
    "\n",
    "        openCV : pip install opencv-python\n",
    "        numpy  : pip install numpy\n",
    "        ipynb  : pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Dataset\n",
    "\n",
    "The Fruits360 dataset will need to be downloaded to the same directory (folder) as this notebook.\n",
    "\n",
    "A zip file (compressed) of the dataset can be obtained at this location:\n",
    "\n",
    "https://pantheon.corp.google.com/storage/browser/cloud-samples-data/air/fruits360/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline Chain\n",
    "\n",
    "The following ML Pipelines will be chained together for this demonstration\n",
    "\n",
    "        frutis360- > openCV -> hdf5 -> model_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Image Files into Machine Learning Data using OpenCV module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the openCV ML pipeline\n",
    "import openCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the on-disk training set of images to in-memory set of machine learning ready data\n",
    "dataset_train = openCV.load_directory('fruits360/Training', colorspace=openCV.COLOR, resize=(100,100), flatten=False, concurrent=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset should be 81 collections (fruits and there varieties).\n",
    "Each collection should consist of a set of three entries: data, labels, and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Number of collections:\", len(dataset_train) )\n",
    "print( \"Number of sets in a collection:\", len(dataset_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first collection should have the label (fruit) 'Avocado' and consist of 427 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images:\", len(dataset_train[0][0]))\n",
    "print(\"Label for collection:\", dataset_train[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Preprocessed Image\", dataset_train[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the on-disk test set of images to in-memory set of machine learning ready data\n",
    "dataset_test = openCV.load_directory('fruits360/Test', colorspace=openCV.COLOR, resize=(100,100), flatten=False, concurrent=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Number of collections:\", len(dataset_test) )\n",
    "print( \"Number of sets in a collection:\", len(dataset_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first collection should have the label (fruit) 'Avocado' and consist of 143 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of images:\", len(dataset_test[0][0]))\n",
    "print(\"Label for collection:\", dataset_test[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Machine Learning Ready (preprocessed images) data into HDF5 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the HDF5 storage ML pipeline\n",
    "import hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the training machine learning ready data to HDF5\n",
    "hdf5.store_dataset('fruits360-training', dataset_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"HDF5 file size:\", int( os.path.getsize('fruits360-training.h5') / (1024 * 1024) ), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the test machine learning ready data to HDF5\n",
    "hdf5.store_dataset('fruits360-test', dataset_test, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"HDF5 file size:\", int( os.path.getsize('fruits360-test.h5') / (1024 * 1024) ), \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct CNN using Keras\n",
    "\n",
    "The CNN below is constructed according to the first method used by Muresan and Oltean in their published paper, with the addition of a dropout layer, which was not used in their paper. Additionally, in their paper they tried other CNN configurations, conversation to grayscale and HSV colorspace and image augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Keras CNN Model ML pipeline\n",
    "import model_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a CNN with input layer of NN of:\n",
    "# Convolutional Layer of 16 filters with input vector (100, 100, 3)\n",
    "# Convolutional Layer of 32 filters\n",
    "# Convolutional Layer of 64 filters\n",
    "# Convolutional Layer of 128 filters\n",
    "# Neural Network Layer of 1024 nodes and 0.25% dropout\n",
    "# Nerual Network Layer of 256 nodes\n",
    "# Output Layer with 81 nodes (classes)\n",
    "model = model_keras.construct_cnn( (100, 100, 3), 81, n_filters=(16, 32, 64, 128), n_nodes=(1024, 256), dropout=(0.25,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset back into memory\n",
    "collections_train, labels_train, classes = hdf5.load_dataset('fruits360-training')\n",
    "collections_test, labels_test, classes = hdf5.load_dataset('fruits360-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\")\n",
    "print(\"Images\", type(collections_train), len(collections_train))\n",
    "print(\"Labels\", type(labels_train), len(labels_train))\n",
    "print(\"Classes\", classes)\n",
    "\n",
    "print(\"Test\")\n",
    "print(\"Images\", type(collections_test), len(collections_test))\n",
    "print(\"Labels\", type(labels_test), len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training (in verbose mode), each epoch will output the current accuracy on the training data (acc) and accuracy on the testing data (val_acc).\n",
    "\n",
    "*Best Practices*\n",
    "1. Once the value of val_acc levels off (stops improving) you should stop training; otherwise the model may overfit.\n",
    "\n",
    "2. If there is a high value for acc and low value for val_acc, the model is likely overfitted. Things to try:\n",
    "        A. Add higher dropout or dropout to more layers.\n",
    "        B. Reduce the number of nodes.\n",
    "        \n",
    "3. If you increase the batch size, the training time per epoch is reduced. Common practice is to set (mini) batch sizes between 32 and 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "accuracy = model_keras.train_cnn(model, collections_train, labels_train, collections_test, labels_test, epochs=10, batch_size=256, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the accuracy\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('fruits360.model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
