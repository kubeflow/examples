{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Google LLC. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a Model for Computer Vision using Tensorflow\n",
    "\n",
    "This *ML pipeline' constructs a Convolutional Neural Network (CNN) using Tensorflow framework, as follows:\n",
    "\n",
    "        1. Configurable # of 2D Convolutional Layer, with configurable input size.\n",
    "        2. Max Pooling and Flattening Layer.\n",
    "        3. Configurable # of Neural Network layers, with configurable number of nodes.\n",
    "        4. Configurable # of dropout Layer, with configurable percentage.\n",
    "        5. Output Layer, with configurable number of outputs (classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow's Neural Network components\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Parameters\n",
    "\n",
    "Use the `construct_cnn()` routine to construct the CNN, which is construct as:\n",
    "\n",
    "                    Convolutional -> Neural Network -> Output\n",
    "\n",
    "The `input_size` is the (height,width) shape of the preprocessed image data (machine learning ready data). For example, in the MNIST and EMMNIST datasets, the input size is (28,28).\n",
    "\n",
    "The `n_classes` is the number of classes to train the model for. Each class is a distinct object to recognize (e.g., a cat). The number of classes will be the number of nodes in the output layer of the neural network. For example, in the MNIST and EMMNIST datasets, the number of classes is 10 and 62 respectively.\n",
    "\n",
    "The `n_nodes` may either be a single integer value or a list of integer values. When specified as a single integer, the value is the number of nodes in the input layer of the neural network from the convolutional front-end, and there are no hidden layers.\n",
    "\n",
    "Otherwise, `n_nodes` is a list, the first list element is the number of nodes in the input layer of the neural network from the convolutional front-end. The remaining elements are the hidden layers, where the value of the element is the number of nodes in the corresponding hidden layer.\n",
    "\n",
    "The `dropout` is the percentage of dropout after the first layer of the neural network. If the value is 0, then there is no dropout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_cnn(input_shape, n_classes, n_filters=32, n_nodes=128, dropout=0):\n",
    "    \"\"\" Construct a CNN model function for instantiating a CNN model using tf.estimator\n",
    "    Args:\n",
    "        input_shape: (tuple(int,int,int)) the 3D shape of the input vector.\n",
    "        n_classes  : (int) total number of classes.\n",
    "        n_filters  : (tuple(int,...)) number of filters per convolutional layer\n",
    "        n_nodes    : (tuple(int,...)) number of nodes per neural network layer.\n",
    "        dropout    : (float) Dropout rate between 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A function which creates a CNN model creation function for the tf.estimator\n",
    "\n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    \n",
    "    def model_cnn_fn(features, labels, mode):\n",
    "        \"\"\" A CNN model creation function for tf.estimator\n",
    "        Args:\n",
    "            features :\n",
    "            labels   :\n",
    "            mode     :\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "        Raises:\n",
    "            None.\n",
    "        \"\"\"\n",
    "\n",
    "        # Constructing a Feed Forward Neural Network\n",
    "        \n",
    "        # Make n_filters a tuple if a single int\n",
    "        if isinstance(n_filters, int):\n",
    "            n_filters = tuple([n_filters])\n",
    "      \n",
    "        # Add a first convolutional front-end with 3x3 kernal\n",
    "        A = _Conv2D(X, filters=n_filters[0], kernel_size=3)\n",
    "        Z = _MaxPooling2D(A, kernel_size=2)\n",
    "\n",
    "        # Add Remaining Convolutional layers\n",
    "        for ix in range(1, len(n_filters)):\n",
    "            # Add next convolutional front-end with 3x3 kernal\n",
    "            A = _Conv2D(Z, filters=n_filters[ix], kernel_size=3) \n",
    "            Z = _MaxPooling2D(A, kernel_size=2)\n",
    "\n",
    "        # Flatten the output from the max pooling layer for input to the neural network\n",
    "        Z = _Flatten(Z)\n",
    "    \n",
    "        # make n_nodes a tuple if a single integer\n",
    "        if isinstance(n_nodes, int):\n",
    "            n_nodes = tuple([n_nodes])\n",
    "        \n",
    "        # make dropout a tuple if a single integer\n",
    "        if isinstance(dropout, int) or isinstance(dropout, float):\n",
    "            # apply dropout to the first layer\n",
    "            dropout = [dropout]\n",
    "            # make remaining layers zero\n",
    "            for _ in range(1, len(n_nodes)):\n",
    "                dropout.append(0)\n",
    "            dropout = tuple(dropout)\n",
    "\n",
    "        # Add layers\n",
    "        for ix in range(len(n_nodes)):\n",
    "            Z = _Dense(Z, n_nodes[ix])\n",
    "            # Add dropout if any])\n",
    "            if dropout[ix] > 0:\n",
    "                Z = _Dropout(Z, dropout[ix], mode)\n",
    "    \n",
    "        # Add the output layer\n",
    "        logits = _Dense(Z, n_classes, activation='softmax')\n",
    "        \n",
    "        predictions = {\n",
    "            # Generate predictions (for PREDICT and EVAL mode)\n",
    "            \"classes\": tf.argmax(input=logits, axis=1),\n",
    "            # Add `softmax_tensor` to the graph. It is used for PREDICT and by the `logging_hook`.\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        }\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "        # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "        # Configure the Training Op (for TRAIN mode)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "            train_op = optimizer.minimize( loss=loss, global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "        # Add evaluation metrics (for EVAL mode)\n",
    "        eval_metric_ops = {\n",
    "          \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "        }\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    \n",
    "    return model_cnn_fn\n",
    "\n",
    "def _Conv2D(T, filters=32, kernel_size=3):\n",
    "    \"\"\" Construct a 2D Convolutional Layer\n",
    "    Args:\n",
    "        T          : input tensor\n",
    "        filters    : number of filters\n",
    "        kernel_size: size of the kernel (e.g., 3 for 3x3)\n",
    "        \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "        \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"  \n",
    "    # Convolutional Layer\n",
    "    A = tf.layers.conv2d(\n",
    "          inputs=T,\n",
    "          filters=filters,\n",
    "          kernel_size=[kernel_size, kernel_size],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "    return A\n",
    "\n",
    "def _MaxPooling2D(T, kernel_size=2):\n",
    "    \"\"\" Construct a 2D Max Pooling Layer\n",
    "    Args:\n",
    "       T          : input tensor\n",
    "       kernel_size: size of the kernel (e.g., 2 for 2x2)\n",
    "       \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "        \n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # The max pooling layer\n",
    "    Z = tf.layers.max_pooling2d(inputs=T, pool_size=[kernel_size, kernel_size], strides=2)\n",
    "    return Z\n",
    "\n",
    "def _Flatten(T):\n",
    "    \"\"\" Flatten a tensor.\n",
    "    Args:\n",
    "        T : input tensor\n",
    "        \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "    \n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    Z = tf.layers.Flatten()(T)\n",
    "    return Z\n",
    "\n",
    "def _Dense(T, n_nodes, activation='relu'):\n",
    "    \"\"\" Construct a Dense Layer\n",
    "    Args:\n",
    "        T      : input tensor\n",
    "        n_nodes: the number of nodes in the layer\n",
    "        \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "        \n",
    "    Raises\n",
    "        None\n",
    "    \"\"\"\n",
    "    if activation == 'relu':\n",
    "        A = tf.layers.dense(inputs=T, units=n_nodes, activation=tf.nn.relu)\n",
    "    else:\n",
    "        A = tf.layers.dense(inputs=T, units=n_nodes)\n",
    "    return A\n",
    "\n",
    "def _Dropout(T, percent, mode):\n",
    "    \"\"\" Construct a dropout layer\n",
    "    Args:\n",
    "        T      : input tensor\n",
    "        percent: the percent of nodes to drop out\n",
    "        \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "        \n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    " \n",
    "    Z = tf.layers.dropout(inputs=T, rate=percent, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function train_cnn() there are two ways of passing the train/test data.\n",
    "\n",
    "1. Unsplit: the train and test data are passed as combined data, in which case x_test and y_test are None. The function will then shuffle the combined data and then split the combined data into training and test based on the percent parameter.\n",
    "\n",
    "2. Split: the train and test data are already split, in which case x and y are the training data and x_test and y_test are the test data. The function does not shuffle or split the pre-split data, and the percent parameter is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def train_cnn(model, x, y, x_test=None, y_test=None, epochs=10, batch_size=32, percent=0.2, verbose=False, seed=113, learning_rate=0.01):\n",
    "    \"\"\" Train the model\n",
    "    Args:\n",
    "        model     : (model_fn) The CNN model creation function.\n",
    "        x         : (numpy.ndarray) The x portion (preprocessed image data) of the dataset.\n",
    "        y         : (numpy.ndarray) The y portion (labels) of the dataset.\n",
    "        x_test    : (numpy.ndarray) The x_test (if pre-split) portion of the dataset.\n",
    "        y_test    : (numpy.ndarray) The y_test (if pre-split) portion of the dataset.\n",
    "        epochs    : (int) The number of times to feed the entire dataset for training.\n",
    "        batch_size: (int) The mini-batch size.\n",
    "        percent   : (float) The percent of the dataset to use for test.\n",
    "        verbose   : (bool) Display (console) progress status.\n",
    "        seed      : (int) Seed for random shuffle before splitting.\n",
    "        \n",
    "    Returns:\n",
    "        The model accuracy after training and evaluation.\n",
    "    \n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the Estimator\n",
    "    classifier = tf.estimator.Estimator(model_fn=model, model_dir=\"./\")\n",
    "    \n",
    "    # Logging Hooks to output progress while training\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "    \n",
    "    # one hot encode the labels\n",
    "    y = np_utils.to_categorical(y)\n",
    "    if y_test is not None:\n",
    "        y_test = np_utils.to_categorical(y_test)\n",
    "   \n",
    "    # Images are grayscale. This model expects shape to be (rows, height, width, channels) vs. (rows, height, width)\n",
    "    if len(x.shape) == 3:\n",
    "        x = x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)\n",
    "        if x_test is not None:\n",
    "            x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "            \n",
    "    # Ignore percent if data is already split\n",
    "    if x_test is not None:\n",
    "        percent = 0\n",
    "    \n",
    "    \n",
    "    # Calculate the number of elements which will be used as training data\n",
    "    train_size = int((1-percent) * len(x))\n",
    "    if verbose: print(\"Training Size:\", train_size)\n",
    "     \n",
    "    # Dataset is combined\n",
    "    if x_test is None:\n",
    "        # Randomly shuffle the data before splitting\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(x)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(y)\n",
    "\n",
    "        # split the data into Train and Test\n",
    "        X_train = x[:train_size]\n",
    "        Y_train = y[:train_size]\n",
    "        X_test  = x[train_size:]\n",
    "        Y_test  = y[train_size:]\n",
    "    # Dataset is presplit\n",
    "    else:\n",
    "        X_train = x\n",
    "        Y_train = y\n",
    "        X_test  = x_test\n",
    "        Y_test  = y_test\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_train},\n",
    "        y=Y_train,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=epochs,\n",
    "        shuffle=True)\n",
    "    \n",
    "    # TODO: Resolve difference between epochs and steps\n",
    "    classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=20000,\n",
    "        hooks=[logging_hook])\n",
    "    \n",
    "    if verbose: print(\"Time\", time.time() - start)\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_test},\n",
    "        y=Y_test,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "    if verbose: print(eval_results)\n",
    "            \n",
    "    \n",
    "    return eval_results\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codeGenerator(input_shape, n_classes, n_filters=32, n_nodes=128, dropout=0):\n",
    "    code = []\n",
    "    code.append(\"def model_cnn_fn(features, labels, mode):\")\n",
    "    \n",
    "    code.append(\"\\t# Constructing a Feed Forward Neural Network\")\n",
    "    code.append(\"\")\n",
    "    \n",
    "    code.append(\"\\ttf.reshape(features['x'], [-1, \" + str(input_shape[0]) + \", \" + str(input_shape[1]) + \", \" + str(input_shape[2]) + \"])\")\n",
    "    code.append(\"\")\n",
    "        \n",
    "    # Make n_filters a tuple if a single int\n",
    "    if isinstance(n_filters, int):\n",
    "        n_filters = tuple([n_filters])\n",
    "      \n",
    "    code.append(\"\\t\" + codeConv2D(filters=n_filters[0], kernel_size=3))\n",
    "        \n",
    "    # Add Remaining Convolutional layers\n",
    "    for ix in range(1, len(n_filters)):\n",
    "        if n_filters[ix] == True:\n",
    "            # Add max pooling layer\n",
    "            code.append(\"\\t\" + codeMaxPooling2D(kernel_size=2))\n",
    "        else:\n",
    "            # Add next convolutional front-end with 3x3 kernal\n",
    "            code.append(\"\\t\" + codeConv2D(filters=n_filters[ix], kernel_size=3))\n",
    "\n",
    "    # Add max pooling layer\n",
    "    code.append(\"\\t\" + codeMaxPooling2D(kernel_size=2))\n",
    "              \n",
    "    # Flatten the output from the max pooling layer for input to the neural network\n",
    "    code.append(\"\\t\" + codeFlatten())\n",
    "    \n",
    "    # make n_nodes a tuple if a single integer\n",
    "    if isinstance(n_nodes, int):\n",
    "        n_nodes = tuple([n_nodes])\n",
    "        \n",
    "    # make dropout a tuple if a single integer\n",
    "    if isinstance(dropout, int) or isinstance(dropout, float):\n",
    "        # apply dropout to the first layer\n",
    "        dropout = [dropout]\n",
    "        # make remaining layers zero\n",
    "        for _ in range(1, len(n_nodes)):\n",
    "            dropout.append(0)\n",
    "        dropout = tuple(dropout)\n",
    "\n",
    "    # Add layers\n",
    "    for ix in range(len(n_nodes)):\n",
    "        code.append(\"\\t\" + codeDense(n_nodes[ix]))\n",
    "        # Add dropout if any])\n",
    "        if dropout[ix] > 0:\n",
    "            code.append(\"\\t\" + codeDropout(dropout[ix]))\n",
    "    \n",
    "    # Add the output layer\n",
    "    code.append(\"\\tlogits = \" + codeDense(n_classes, activation='softmax'))\n",
    "    \n",
    "    code.append(\"\")\n",
    "    code.append(\"\\tpredictions = {\")\n",
    "    code.append(\"\\t\\t# Generate predictions (for PREDICT and EVAL mode)\")\n",
    "    code.append(\"\\t\\t'classes': tf.argmax(input=logits, axis=1),\")\n",
    "    code.append(\"\\t\\t# Add `softmax_tensor` to the graph. It is used for PREDICT and by the `logging_hook`.\")\n",
    "    code.append(\"\\t\\t'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\")\n",
    "    code.append(\"\\t}\")  \n",
    "    \n",
    "    code.append(\"\")\n",
    "    code.append(\"\\tif mode == tf.estimator.ModeKeys.PREDICT:\")\n",
    "    code.append(\"\\t\\treturn tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\")\n",
    "\n",
    "    code.append(\"\")\n",
    "    code.append(\"\\t# Calculate Loss (for both TRAIN and EVAL modes)\")\n",
    "    code.append(\"\\tloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\")\n",
    "    \n",
    "    code.append(\"\")\n",
    "    code.append(\"\\t# Configure the Training Op (for TRAIN mode)\")\n",
    "    code.append(\"\\tif mode == tf.estimator.ModeKeys.TRAIN:\")\n",
    "    code.append(\"\\t\\toptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\")\n",
    "    code.append(\"\\t\\ttrain_op = optimizer.minimize( loss=loss, global_step=tf.train.get_global_step())\")\n",
    "    code.append(\"\\t\\treturn tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\")\n",
    "    \n",
    "    code.append(\"\")\n",
    "    code.append(\"\\t# Add evaluation metrics (for EVAL mode)\")\n",
    "    code.append(\"\\teval_metric_ops = {\")\n",
    "    code.append(\"\\t\\t'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions['classes'])\")\n",
    "    code.append(\"\\t}\")\n",
    "        \n",
    "    code.append(\"\")\n",
    "    code.append(\"\\treturn tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\")\n",
    "            \n",
    "    for line in code:\n",
    "        print(line)\n",
    "                \n",
    "def codeConv2D(filters=32, kernel_size=3):\n",
    "    \"\"\" Construct a 2D Convolutional Layer\n",
    "    Args:\n",
    "        filters    : number of filters\n",
    "        kernel_size: size of the kernel (e.g., 3 for 3x3)\n",
    "        \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "        \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"  \n",
    "    # Convolutional Layer\n",
    "    s = 'T = tf.layers.conv2d(inputs=T, filters=' + str(filters) + ', kernel_size=[' + str(kernel_size) + ', ' + str(kernel_size) + '], padding=\"same\",activation=tf.nn.relu)'\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def codeMaxPooling2D(kernel_size=2):\n",
    "    \"\"\" Construct a 2D Max Pooling Layer\n",
    "    Args:\n",
    "       kernel_size: size of the kernel (e.g., 2 for 2x2)\n",
    "       \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "        \n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # The max pooling layer\n",
    "    s = 'T = tf.layers.max_pooling2d(inputs=T, pool_size=[' + str(kernel_size) + ', ' + str(kernel_size) + '], strides=2)'\n",
    "    return s\n",
    "\n",
    "def codeFlatten():\n",
    "    \"\"\" Flatten a tensor.\n",
    "    Args:\n",
    "        \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "    \n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s = 'T = tf.layers.Flatten()(T)'\n",
    "    return s\n",
    "\n",
    "def codeDense(n_nodes, activation='relu'):\n",
    "    \"\"\" Construct a Dense Layer\n",
    "    Args:\n",
    "        n_nodes: the number of nodes in the layer\n",
    "        \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "        \n",
    "    Raises\n",
    "        None\n",
    "    \"\"\"\n",
    "    if activation == 'relu':\n",
    "        s = 'T = tf.layers.dense(inputs=T, units=' + str(n_nodes) + ', activation=tf.nn.relu)'\n",
    "    else:\n",
    "        s = 'T = tf.layers.dense(inputs=T, units=' + str(n_nodes) + ')'\n",
    "    return s\n",
    "\n",
    "def codeDropout(percent):\n",
    "    \"\"\" Construct a dropout layer\n",
    "    Args:\n",
    "        percent: the percent of nodes to drop out\n",
    "        \n",
    "    Returns:\n",
    "        An output tensor.\n",
    "        \n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    " \n",
    "    s = 'T = tf.layers.dropout(inputs=T, rate=' + str(percent) + ', training=mode == tf.estimator.ModeKeys.TRAIN)'\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model_cnn_fn(features, labels, mode):\n",
      "\t# Constructing a Feed Forward Neural Network\n",
      "\n",
      "\ttf.reshape(features['x'], [-1, 28, 28, 1])\n",
      "\n",
      "\tT = tf.layers.conv2d(inputs=T, filters=64, kernel_size=[3, 3], padding=\"same\",activation=tf.nn.relu)\n",
      "\tT = tf.layers.max_pooling2d(inputs=T, pool_size=[2, 2], strides=2)\n",
      "\tT = tf.layers.conv2d(inputs=T, filters=32, kernel_size=[3, 3], padding=\"same\",activation=tf.nn.relu)\n",
      "\tT = tf.layers.max_pooling2d(inputs=T, pool_size=[2, 2], strides=2)\n",
      "\tT = tf.layers.Flatten()(T)\n",
      "\tT = tf.layers.dense(inputs=T, units=512, activation=tf.nn.relu)\n",
      "\tT = tf.layers.dropout(inputs=T, rate=0.5, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
      "\tT = tf.layers.dense(inputs=T, units=20, activation=tf.nn.relu)\n",
      "\tlogits = T = tf.layers.dense(inputs=T, units=10)\n",
      "\n",
      "\tpredictions = {\n",
      "\t\t# Generate predictions (for PREDICT and EVAL mode)\n",
      "\t\t'classes': tf.argmax(input=logits, axis=1),\n",
      "\t\t# Add `softmax_tensor` to the graph. It is used for PREDICT and by the `logging_hook`.\n",
      "\t\t'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
      "\t}\n",
      "\n",
      "\tif mode == tf.estimator.ModeKeys.PREDICT:\n",
      "\t\treturn tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
      "\n",
      "\t# Calculate Loss (for both TRAIN and EVAL modes)\n",
      "\tloss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
      "\n",
      "\t# Configure the Training Op (for TRAIN mode)\n",
      "\tif mode == tf.estimator.ModeKeys.TRAIN:\n",
      "\t\toptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
      "\t\ttrain_op = optimizer.minimize( loss=loss, global_step=tf.train.get_global_step())\n",
      "\t\treturn tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
      "\n",
      "\t# Add evaluation metrics (for EVAL mode)\n",
      "\teval_metric_ops = {\n",
      "\t\t'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions['classes'])\n",
      "\t}\n",
      "\n",
      "\treturn tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
     ]
    }
   ],
   "source": [
    "codeGenerator((28,28,1), 10, n_filters=(64, True, 32), n_nodes=(512, 20), dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
